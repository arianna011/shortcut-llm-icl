@misc{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@misc{xie2022explanationincontextlearningimplicit,
      title={An Explanation of In-context Learning as Implicit Bayesian Inference}, 
      author={Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},
      year={2022},
      eprint={2111.02080},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2111.02080}, 
}

@misc{hendrycks2020pretrainedtransformersimproveoutofdistribution,
      title={Pretrained Transformers Improve Out-of-Distribution Robustness}, 
      author={Dan Hendrycks and Xiaoyuan Liu and Eric Wallace and Adam Dziedzic and Rishabh Krishnan and Dawn Song},
      year={2020},
      eprint={2004.06100},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.06100}, 
}

@misc{wallace2021universaladversarialtriggersattacking,
      title={Universal Adversarial Triggers for Attacking and Analyzing NLP}, 
      author={Eric Wallace and Shi Feng and Nikhil Kandpal and Matt Gardner and Sameer Singh},
      year={2021},
      eprint={1908.07125},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1908.07125}, 
}

@article{Geirhos_2020,
   title={Shortcut learning in deep neural networks},
   volume={2},
   ISSN={2522-5839},
   url={http://dx.doi.org/10.1038/s42256-020-00257-z},
   DOI={10.1038/s42256-020-00257-z},
   number={11},
   journal={Nature Machine Intelligence},
   publisher={Springer Science and Business Media LLC},
   author={Geirhos, Robert and Jacobsen, Jörn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
   year={2020},
   month=nov, pages={665–673} }

@misc{bolukbasi2016mancomputerprogrammerwoman,
      title={Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings}, 
      author={Tolga Bolukbasi and Kai-Wei Chang and James Zou and Venkatesh Saligrama and Adam Kalai},
      year={2016},
      eprint={1607.06520},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1607.06520}, 
}

@misc{seshadri2025smallchangeslargeconsequences,
      title={Small Changes, Large Consequences: Analyzing the Allocational Fairness of LLMs in Hiring Contexts}, 
      author={Preethi Seshadri and Hongyu Chen and Sameer Singh and Seraphina Goldfarb-Tarrant},
      year={2025},
      eprint={2501.04316},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.04316}, 
}

@inproceedings{bender2021danger,
author = {Bender, Emily and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
year = {2021},
month = {03},
pages = {610-623},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
doi = {10.1145/3442188.3445922}
}

@article{turing,
    author = {Turing, A. M.},
    title = {I.—COMPUTING MACHINERY AND INTELLIGENCE},
    journal = {Mind},
    volume = {LIX},
    number = {236},
    pages = {433-460},
    year = {1950},
    month = {10},
    issn = {0026-4423},
    doi = {10.1093/mind/LIX.236.433},
    url = {https://doi.org/10.1093/mind/LIX.236.433},
    eprint = {https://academic.oup.com/mind/article-pdf/LIX/236/433/61209000/mind_lix_236_433.pdf},
}

@article{hutchins2005georgetown,
  title={The first public demonstration of machine translation: the Georgetown-IBM system, 7th January 1954},
  author={Hutchins, John},
  journal={arXiv preprint cs/0506077},
  year={2005}
}

@book{arnold1994machine,
  title={Machine Translation: An Introductory Guide},
  author={Arnold, Douglas and Balkan, Lorna and Meijer, Siety and Humphreys, R. Lee and Sadler, Louisa},
  year={1994},
  publisher={Blackwells-NCC}
}

@inproceedings{chandioux1976meteo,
  title={METEO: System for Automatic Translation of Weather Forecasts},
  author={Chandioux, John},
  booktitle={Conference of the American Translators Association},
  year={1976}
}

@article{weizenbaum1966eliza,
  title={ELIZA—a computer program for the study of natural language communication between man and machine},
  author={Weizenbaum, Joseph},
  journal={Communications of the ACM},
  volume={9},
  number={1},
  pages={36--45},
  year={1966},
  publisher={ACM}
}

@inproceedings{marcus1993building,
  title={Building a large annotated corpus of English: The Penn Treebank},
  author={Marcus, Mitchell P and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
  booktitle={Computational linguistics},
  volume={19},
  number={2},
  pages={313--330},
  year={1993},
  publisher={MIT Press}
}

@book{francis1964manual,
  title={A Manual of Information to Accompany A Standard Corpus of Present-Day Edited American English, for Use with Digital Computers},
  author={Francis, W. Nelson and Kučera, Henry},
  year={1964},
  publisher={Department of Linguistics, Brown University}
}

@incollection{jelinek1990self,
  title={Self-organized language modeling for speech recognition},
  author={Jelinek, Frederick},
  booktitle={Readings in speech recognition},
  pages={450--506},
  year={1990},
  publisher={Morgan Kaufmann}
}

@inproceedings{kupiec1992robust,
  title={Robust part-of-speech tagging using a hidden Markov model},
  author={Kupiec, Julian},
  booktitle={Proceedings of the conference on Artificial intelligence},
  pages={189--194},
  year={1992}
}

@article{berger1996maximum,
  title={A maximum entropy approach to natural language processing},
  author={Berger, Adam L and Pietra, Vincent J Della and Pietra, Stephen A Della},
  journal={Computational linguistics},
  volume={22},
  number={1},
  pages={39--71},
  year={1996},
  publisher={MIT Press}
}

@misc{beery2018recognitionterraincognita,
      title={Recognition in Terra Incognita}, 
      author={Sara Beery and Grant van Horn and Pietro Perona},
      year={2018},
      eprint={1807.04975},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1807.04975}, 
}

@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{jiang2024manyshotincontextlearningmultimodal,
      title={Many-Shot In-Context Learning in Multimodal Foundation Models}, 
      author={Yixing Jiang and Jeremy Irvin and Ji Hun Wang and Muhammad Ahmed Chaudhry and Jonathan H. Chen and Andrew Y. Ng},
      year={2024},
      eprint={2405.09798},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.09798}, 
}

@misc{chen2024iclevalevaluatingincontextlearning,
      title={ICLEval: Evaluating In-Context Learning Ability of Large Language Models}, 
      author={Wentong Chen and Yankai Lin and ZhenHao Zhou and HongYun Huang and Yantao Jia and Zhao Cao and Ji-Rong Wen},
      year={2024},
      eprint={2406.14955},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.14955}, 
}

@misc{milios2023incontextlearningtextclassification,
      title={In-Context Learning for Text Classification with Many Labels}, 
      author={Aristides Milios and Siva Reddy and Dzmitry Bahdanau},
      year={2023},
      eprint={2309.10954},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.10954}, 
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{dong2024surveyincontextlearning,
      title={A Survey on In-context Learning}, 
      author={Qingxiu Dong and Lei Li and Damai Dai and Ce Zheng and Jingyuan Ma and Rui Li and Heming Xia and Jingjing Xu and Zhiyong Wu and Tianyu Liu and Baobao Chang and Xu Sun and Lei Li and Zhifang Sui},
      year={2024},
      eprint={2301.00234},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2301.00234}, 
}

@inproceedings{shin-etal-2022-effect,
    title = "On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model",
    author = "Shin, Seongjin  and
      Lee, Sang-Woo  and
      Ahn, Hwijeen  and
      Kim, Sungdong  and
      Kim, HyoungSeok  and
      Kim, Boseop  and
      Cho, Kyunghyun  and
      Lee, Gichang  and
      Park, Woomyoung  and
      Ha, Jung-Woo  and
      Sung, Nako",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.380/",
    doi = "10.18653/v1/2022.naacl-main.380",
    pages = "5168--5186",
    abstract = "Many recent studies on large-scale language models have reported successful in-context zero- and few-shot learning ability. However, the in-depth analysis of when in-context learning occurs is still lacking. For example, it is unknown how in-context learning performance changes as the training corpus varies. Here, we investigate the effects of the source and size of the pretraining corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From our in-depth investigation, we introduce the following observations: (1) in-context learning performance heavily depends on the corpus domain source, and the size of the pretraining corpus does not necessarily determine the emergence of in-context learning, (2) in-context learning ability can emerge when a language model is trained on a combination of multiple corpora, even when each corpus does not result in in-context learning on its own, (3) pretraining with a corpus related to a downstream task does not always guarantee the competitive in-context learning performance of the downstream task, especially in the few-shot setting, and (4) the relationship between language modeling (measured in perplexity) and in-context learning does not always correlate: e.g., low perplexity does not always imply high in-context few-shot learning performance."
}

@misc{wei2022emergentabilitieslargelanguage,
      title={Emergent Abilities of Large Language Models}, 
      author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
      year={2022},
      eprint={2206.07682},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2206.07682}, 
}

@misc{min2022rethinkingroledemonstrationsmakes,
      title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?}, 
      author={Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
      year={2022},
      eprint={2202.12837},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2202.12837}, 
}

@misc{liu2021makesgoodincontextexamples,
      title={What Makes Good In-Context Examples for GPT-$3$?}, 
      author={Jiachang Liu and Dinghan Shen and Yizhe Zhang and Bill Dolan and Lawrence Carin and Weizhu Chen},
      year={2021},
      eprint={2101.06804},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2101.06804}, 
}

@misc{lu2022fantasticallyorderedpromptsthem,
      title={Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity}, 
      author={Yao Lu and Max Bartolo and Alastair Moore and Sebastian Riedel and Pontus Stenetorp},
      year={2022},
      eprint={2104.08786},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.08786}, 
}

@misc{si2023measuringinductivebiasesincontext,
      title={Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations}, 
      author={Chenglei Si and Dan Friedman and Nitish Joshi and Shi Feng and Danqi Chen and He He},
      year={2023},
      eprint={2305.13299},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13299}, 
}

@misc{pan2023incontextlearninglearnsincontext,
      title={What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning}, 
      author={Jane Pan and Tianyu Gao and Howard Chen and Danqi Chen},
      year={2023},
      eprint={2305.09731},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.09731}, 
}

@misc{zhang2023doesincontextlearninglearn,
      title={What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization}, 
      author={Yufeng Zhang and Fengzhuo Zhang and Zhuoran Yang and Zhaoran Wang},
      year={2023},
      eprint={2305.19420},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2305.19420}, 
}

@misc{akyürek2023learningalgorithmincontextlearning,
      title={What learning algorithm is in-context learning? Investigations with linear models}, 
      author={Ekin Akyürek and Dale Schuurmans and Jacob Andreas and Tengyu Ma and Denny Zhou},
      year={2023},
      eprint={2211.15661},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.15661}, 
}

@misc{vonoswald2023transformerslearnincontextgradient,
      title={Transformers learn in-context by gradient descent}, 
      author={Johannes von Oswald and Eyvind Niklasson and Ettore Randazzo and João Sacramento and Alexander Mordvintsev and Andrey Zhmoginov and Max Vladymyrov},
      year={2023},
      eprint={2212.07677},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.07677}, 
}

@inproceedings{niven-kao-2019-probing,
    title = "Probing Neural Network Comprehension of Natural Language Arguments",
    author = "Niven, Timothy  and
      Kao, Hung-Yu",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1459/",
    doi = "10.18653/v1/P19-1459",
    pages = "4658--4664",
    abstract = "We are surprised to find that BERT{'}s peak performance of 77{\%} on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work."
}

@inproceedings{Murphy2013TheFL,
  title={The First Level of Super Mario Bros . is Easy with Lexicographic Orderings and Time Travel},
  author={Dr. Tom Murphy},
  year={2013},
  url={https://api.semanticscholar.org/CorpusID:14347703}
}

@misc{zhao2017menlikeshoppingreducing,
      title={Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints}, 
      author={Jieyu Zhao and Tianlu Wang and Mark Yatskar and Vicente Ordonez and Kai-Wei Chang},
      year={2017},
      eprint={1707.09457},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1707.09457}, 
}

@article{Zech2018VariableGP,
  title={Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study},
  author={John R. Zech and Marcus A. Badgeley and Manway Liu and Anthony Beardsworth Costa and Joseph J. Titano and Eric Karl Oermann},
  journal={PLoS Medicine},
  year={2018},
  volume={15},
  url={https://api.semanticscholar.org/CorpusID:49558635}
}

@book{cleverhans,
	title = {Clever Hans (the horse of Mr. Von Osten) a contribution to experimental animal and human psychology  },
	copyright = {Public domain. The BHL considers that this work is no longer under copyright protection.},
	url = {https://www.biodiversitylibrary.org/item/116908},
	note = {https://www.biodiversitylibrary.org/bibliography/56164 --- "Table of references": p. 267-274. --- Supplements i-iv: p. 245-265.},
	publisher = {New York, H. Holt and company, 1911},
	author = {Pfungst, Oskar. and Rahn, Carl Leo.},
	year = {1911},
	pages = {296},
	keywords = {Animal intelligence|Animal training|Horses|Psychology, Comparative|},
}

@misc{steinmann2024navigatingshortcutsspuriouscorrelations,
      title={Navigating Shortcuts, Spurious Correlations, and Confounders: From Origins via Detection to Mitigation}, 
      author={David Steinmann and Felix Divo and Maurice Kraus and Antonia Wüst and Lukas Struppek and Felix Friedrich and Kristian Kersting},
      year={2024},
      eprint={2412.05152},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.05152}, 
}

@misc{vallepérez2019deeplearninggeneralizesparameterfunction,
      title={Deep learning generalizes because the parameter-function map is biased towards simple functions}, 
      author={Guillermo Valle-Pérez and Chico Q. Camargo and Ard A. Louis},
      year={2019},
      eprint={1805.08522},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1805.08522}, 
}

@misc{song2024shortcutlearningincontextlearning,
      title={Shortcut Learning in In-Context Learning: A Survey}, 
      author={Rui Song and Yingji Li and Lida Shi and Fausto Giunchiglia and Hao Xu},
      year={2024},
      eprint={2411.02018},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.02018}, 
}

@misc{du2023shortcutlearninglargelanguage,
      title={Shortcut Learning of Large Language Models in Natural Language Understanding}, 
      author={Mengnan Du and Fengxiang He and Na Zou and Dacheng Tao and Xia Hu},
      year={2023},
      eprint={2208.11857},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2208.11857}, 
}

@misc{jin2020bertreallyrobuststrong,
      title={Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment}, 
      author={Di Jin and Zhijing Jin and Joey Tianyi Zhou and Peter Szolovits},
      year={2020},
      eprint={1907.11932},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.11932}, 
}

@misc{yang2023gluexevaluatingnaturallanguage,
      title={GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective}, 
      author={Linyi Yang and Shuibai Zhang and Libo Qin and Yafu Li and Yidong Wang and Hanmeng Liu and Jindong Wang and Xing Xie and Yue Zhang},
      year={2023},
      eprint={2211.08073},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.08073}, 
}

@misc{pham2021orderimportantsequentialorder,
      title={Out of Order: How Important Is The Sequential Order of Words in a Sentence in Natural Language Understanding Tasks?}, 
      author={Thang M. Pham and Trung Bui and Long Mai and Anh Nguyen},
      year={2021},
      eprint={2012.15180},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2012.15180}, 
}

@misc{du2021interpretingmitigatingshortcutlearning,
      title={Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU Models}, 
      author={Mengnan Du and Varun Manjunatha and Rajiv Jain and Ruchi Deshpande and Franck Dernoncourt and Jiuxiang Gu and Tong Sun and Xia Hu},
      year={2021},
      eprint={2103.06922},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2103.06922}, 
}

@misc{han2020explainingblackboxpredictions,
      title={Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions}, 
      author={Xiaochuang Han and Byron C. Wallace and Yulia Tsvetkov},
      year={2020},
      eprint={2005.06676},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.06676}, 
}

@misc{shi2021gradientmatchingdomaingeneralization,
      title={Gradient Matching for Domain Generalization}, 
      author={Yuge Shi and Jeffrey Seely and Philip H. S. Torr and N. Siddharth and Awni Hannun and Nicolas Usunier and Gabriel Synnaeve},
      year={2021},
      eprint={2104.09937},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2104.09937}, 
}

@inproceedings{utama-etal-2020-towards,
    title = "Towards Debiasing {NLU} Models from Unknown Biases",
    author = "Utama, Prasetya Ajie  and
      Moosavi, Nafise Sadat  and
      Gurevych, Iryna",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.613/",
    doi = "10.18653/v1/2020.emnlp-main.613",
    pages = "7597--7610"
}

@misc{li2024examiningforgettingcontinualpretraining,
      title={Examining Forgetting in Continual Pre-training of Aligned Large Language Models}, 
      author={Chen-An Li and Hung-Yi Lee},
      year={2024},
      eprint={2401.03129},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.03129}, 
}

@inproceedings{stacey-etal-2020-avoiding,
    title = "{A}voiding the {H}ypothesis-{O}nly {B}ias in {N}atural {L}anguage {I}nference via {E}nsemble {A}dversarial {T}raining",
    author = {Stacey, Joe  and
      Minervini, Pasquale  and
      Dubossarsky, Haim  and
      Riedel, Sebastian  and
      Rockt{\"a}schel, Tim},
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.665/",
    doi = "10.18653/v1/2020.emnlp-main.665",
    pages = "8281--8291"
}

@misc{stacey2022supervisingmodelattentionhuman,
      title={Supervising Model Attention with Human Explanations for Robust Natural Language Inference}, 
      author={Joe Stacey and Yonatan Belinkov and Marek Rei},
      year={2022},
      eprint={2104.08142},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.08142}, 
}


@misc{clark2019donteasywayout,
      title={Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases}, 
      author={Christopher Clark and Mark Yatskar and Luke Zettlemoyer},
      year={2019},
      eprint={1909.03683},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.03683}, 
}

@article{choi2022,
author = {Choi, Seungtaek and Jeong, Myeongho and Han, Hojae and Hwang, Seung-won},
year = {2022},
month = {06},
pages = {10526-10534},
title = {C2L: Causally Contrastive Learning for Robust Text Classification},
volume = {36},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v36i10.21296}
}

@misc{zhou2024unibiasunveilingmitigatingllm,
      title={UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation}, 
      author={Hanzhang Zhou and Zijian Feng and Zixiao Zhu and Junlang Qian and Kezhi Mao},
      year={2024},
      eprint={2405.20612},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.20612}, 
}

@misc{yuan2024llmsovercomeshortcutlearning,
      title={Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models}, 
      author={Yu Yuan and Lili Zhao and Kai Zhang and Guangting Zheng and Qi Liu},
      year={2024},
      eprint={2410.13343},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.13343}, 
}

@misc{zou2025representationengineeringtopdownapproach,
      title={Representation Engineering: A Top-Down Approach to AI Transparency}, 
      author={Andy Zou and Long Phan and Sarah Chen and James Campbell and Phillip Guo and Richard Ren and Alexander Pan and Xuwang Yin and Mantas Mazeika and Ann-Kathrin Dombrowski and Shashwat Goel and Nathaniel Li and Michael J. Byun and Zifan Wang and Alex Mallen and Steven Basart and Sanmi Koyejo and Dawn Song and Matt Fredrikson and J. Zico Kolter and Dan Hendrycks},
      year={2025},
      eprint={2310.01405},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.01405}, 
}

@article{cognitivescience,
author = {Barack, David and Krakauer, John},
year = {2021},
month = {04},
pages = {},
title = {Two views on the cognitive brain},
volume = {22},
journal = {Nature Reviews Neuroscience},
doi = {10.1038/s41583-021-00448-6}
}

@misc{tenney2019bertrediscoversclassicalnlp,
      title={BERT Rediscovers the Classical NLP Pipeline}, 
      author={Ian Tenney and Dipanjan Das and Ellie Pavlick},
      year={2019},
      eprint={1905.05950},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1905.05950}, 
}

@misc{azaria2023internalstatellmknows,
      title={The Internal State of an LLM Knows When It's Lying}, 
      author={Amos Azaria and Tom Mitchell},
      year={2023},
      eprint={2304.13734},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.13734}, 
}

@misc{jiang2023mistral7b,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}


@misc{survey2022,
  author={Alharahseheh, Yara and Obeidat, Rasha and Al-Ayoub, Mahmoud and Gharaibeh, Maram},
  booktitle={2022 13th International Conference on Information and Communication Systems (ICICS)}, 
  title={A Survey on Textual Entailment: Benchmarks, Approaches and Applications}, 
  year={2022},
  volume={},
  number={},
  pages={328-336},
  keywords={Communication systems;Semantics;Benchmark testing;Information retrieval;Natural language processing;Task analysis;Commonsense reasoning;Survey;textual entailment;natural language inference;rule-based;machine learning;deep learning},
  doi={10.1109/ICICS55353.2022.9811200}}

  @misc{williams2018broadcoveragechallengecorpussentence,
      title={A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference}, 
      author={Adina Williams and Nikita Nangia and Samuel R. Bowman},
      year={2018},
      eprint={1704.05426},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1704.05426}, 
}

@inproceedings{dagan2006rte,
  title={The PASCAL Recognising Textual Entailment Challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine Learning Challenges Workshop},
  year={2006}
}

@inproceedings{gordon-etal-2012-semeval,
    title = "{S}em{E}val-2012 Task 7: Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning",
    author = "Gordon, Andrew  and
      Kozareva, Zornitsa  and
      Roemmele, Melissa",
    editor = "Agirre, Eneko  and
      Bos, Johan  and
      Diab, Mona  and
      Manandhar, Suresh  and
      Marton, Yuval  and
      Yuret, Deniz",
    booktitle = "*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",
    month = "7-8 " # jun,
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S12-1052/",
    pages = "394--398"
}

@misc{turney2002thumbsthumbsdownsemantic,
      title={Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews}, 
      author={Peter D. Turney},
      year={2002},
      eprint={cs/0212032},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/cs/0212032}, 
}

@inproceedings{socher-etal-2013-recursive,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    editor = "Yarowsky, David  and
      Baldwin, Timothy  and
      Korhonen, Anna  and
      Livescu, Karen  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1170/",
    pages = "1631--1642"
}

@inproceedings{pilehvar-camacho-collados-2019-wic,
    title = "{W}i{C}: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations",
    author = "Pilehvar, Mohammad Taher  and
      Camacho-Collados, Jose",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1128/",
    doi = "10.18653/v1/N19-1128",
    pages = "1267--1273",
    abstract = "By design, word embeddings are unable to model the dynamic nature of words' semantics, i.e., the property of words to correspond to potentially different meanings. To address this limitation, dozens of specialized meaning representation techniques such as sense or contextualized embeddings have been proposed. However, despite the popularity of research on this topic, very few evaluation benchmarks exist that specifically focus on the dynamic semantics of words. In this paper we show that existing models have surpassed the performance ceiling of the standard evaluation dataset for the purpose, i.e., Stanford Contextual Word Similarity, and highlight its shortcomings. To address the lack of a suitable benchmark, we put forward a large-scale Word in Context dataset, called WiC, based on annotations curated by experts, for generic evaluation of context-sensitive representations. WiC is released in \url{https://pilehvar.github.io/wic/}."
}

@misc{clark2018thinksolvedquestionanswering,
      title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge}, 
      author={Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
      year={2018},
      eprint={1803.05457},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1803.05457}, 
}

@misc{hendrycks2021measuringmassivemultitasklanguage,
      title={Measuring Massive Multitask Language Understanding}, 
      author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2009.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2009.03300}, 
}

@misc{elhage2022toymodelssuperposition,
      title={Toy Models of Superposition}, 
      author={Nelson Elhage and Tristan Hume and Catherine Olsson and Nicholas Schiefer and Tom Henighan and Shauna Kravec and Zac Hatfield-Dodds and Robert Lasenby and Dawn Drain and Carol Chen and Roger Grosse and Sam McCandlish and Jared Kaplan and Dario Amodei and Martin Wattenberg and Christopher Olah},
      year={2022},
      eprint={2209.10652},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.10652}, 
}

@article{elazar-etal-2021-amnesic,
    title = "Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals",
    author = "Elazar, Yanai  and
      Ravfogel, Shauli  and
      Jacovi, Alon  and
      Goldberg, Yoav",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.10/",
    doi = "10.1162/tacl_a_00359",
    pages = "160--175",
    abstract = "A growing body of work makes use of probing in order to investigate the working of neural models, often considered black boxes. Recently, an ongoing debate emerged surrounding the limitations of the probing paradigm. In this work, we point out the inability to infer behavioral conclusions from probing results, and offer an alternative method that focuses on how the information is being used, rather than on what information is encoded. Our method, Amnesic Probing, follows the intuition that the utility of a property for a given task can be assessed by measuring the influence of a causal intervention that removes it from the representation. Equipped with this new analysis tool, we can ask questions that were not possible before, for example, is part-of-speech information important for word prediction? We perform a series of analyses on BERT to answer these types of questions. Our findings demonstrate that conventional probing performance is not correlated to task importance, and we call for increased scrutiny of claims that draw behavioral or causal conclusions from probing results.1"
}

@article{rogers-etal-2020-primer,
    title = "A Primer in {BERT}ology: What We Know About How {BERT} Works",
    author = "Rogers, Anna  and
      Kovaleva, Olga  and
      Rumshisky, Anna",
    editor = "Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.54/",
    doi = "10.1162/tacl_a_00349",
    pages = "842--866",
    abstract = "Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research."
}

@misc{sundararajan2017axiomaticattributiondeepnetworks,
      title={Axiomatic Attribution for Deep Networks}, 
      author={Mukund Sundararajan and Ankur Taly and Qiqi Yan},
      year={2017},
      eprint={1703.01365},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1703.01365}, 
}

@article{Facco_2017,
   title={Estimating the intrinsic dimension of datasets by a minimal neighborhood information},
   volume={7},
   ISSN={2045-2322},
   url={http://dx.doi.org/10.1038/s41598-017-11873-y},
   DOI={10.1038/s41598-017-11873-y},
   number={1},
   journal={Scientific Reports},
   publisher={Springer Science and Business Media LLC},
   author={Facco, Elena and d’Errico, Maria and Rodriguez, Alex and Laio, Alessandro},
   year={2017},
   month=sep }