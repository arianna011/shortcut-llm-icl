@misc{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@misc{xie2022explanationincontextlearningimplicit,
      title={An Explanation of In-context Learning as Implicit Bayesian Inference}, 
      author={Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},
      year={2022},
      eprint={2111.02080},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2111.02080}, 
}

@misc{hendrycks2020pretrainedtransformersimproveoutofdistribution,
      title={Pretrained Transformers Improve Out-of-Distribution Robustness}, 
      author={Dan Hendrycks and Xiaoyuan Liu and Eric Wallace and Adam Dziedzic and Rishabh Krishnan and Dawn Song},
      year={2020},
      eprint={2004.06100},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.06100}, 
}

@misc{wallace2021universaladversarialtriggersattacking,
      title={Universal Adversarial Triggers for Attacking and Analyzing NLP}, 
      author={Eric Wallace and Shi Feng and Nikhil Kandpal and Matt Gardner and Sameer Singh},
      year={2021},
      eprint={1908.07125},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1908.07125}, 
}

@article{Geirhos_2020,
   title={Shortcut learning in deep neural networks},
   volume={2},
   ISSN={2522-5839},
   url={http://dx.doi.org/10.1038/s42256-020-00257-z},
   DOI={10.1038/s42256-020-00257-z},
   number={11},
   journal={Nature Machine Intelligence},
   publisher={Springer Science and Business Media LLC},
   author={Geirhos, Robert and Jacobsen, Jörn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
   year={2020},
   month=nov, pages={665–673} }

@misc{bolukbasi2016mancomputerprogrammerwoman,
      title={Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings}, 
      author={Tolga Bolukbasi and Kai-Wei Chang and James Zou and Venkatesh Saligrama and Adam Kalai},
      year={2016},
      eprint={1607.06520},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1607.06520}, 
}

@misc{seshadri2025smallchangeslargeconsequences,
      title={Small Changes, Large Consequences: Analyzing the Allocational Fairness of LLMs in Hiring Contexts}, 
      author={Preethi Seshadri and Hongyu Chen and Sameer Singh and Seraphina Goldfarb-Tarrant},
      year={2025},
      eprint={2501.04316},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.04316}, 
}

@inproceedings{bender2021danger,
author = {Bender, Emily and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
year = {2021},
month = {03},
pages = {610-623},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
doi = {10.1145/3442188.3445922}
}

@article{turing,
    author = {Turing, A. M.},
    title = {I.—COMPUTING MACHINERY AND INTELLIGENCE},
    journal = {Mind},
    volume = {LIX},
    number = {236},
    pages = {433-460},
    year = {1950},
    month = {10},
    issn = {0026-4423},
    doi = {10.1093/mind/LIX.236.433},
    url = {https://doi.org/10.1093/mind/LIX.236.433},
    eprint = {https://academic.oup.com/mind/article-pdf/LIX/236/433/61209000/mind_lix_236_433.pdf},
}

@article{hutchins2005georgetown,
  title={The first public demonstration of machine translation: the Georgetown-IBM system, 7th January 1954},
  author={Hutchins, John},
  journal={arXiv preprint cs/0506077},
  year={2005}
}

@book{arnold1994machine,
  title={Machine Translation: An Introductory Guide},
  author={Arnold, Douglas and Balkan, Lorna and Meijer, Siety and Humphreys, R. Lee and Sadler, Louisa},
  year={1994},
  publisher={Blackwells-NCC}
}

@inproceedings{chandioux1976meteo,
  title={METEO: System for Automatic Translation of Weather Forecasts},
  author={Chandioux, John},
  booktitle={Conference of the American Translators Association},
  year={1976}
}

@article{weizenbaum1966eliza,
  title={ELIZA—a computer program for the study of natural language communication between man and machine},
  author={Weizenbaum, Joseph},
  journal={Communications of the ACM},
  volume={9},
  number={1},
  pages={36--45},
  year={1966},
  publisher={ACM}
}

@inproceedings{marcus1993building,
  title={Building a large annotated corpus of English: The Penn Treebank},
  author={Marcus, Mitchell P and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
  booktitle={Computational linguistics},
  volume={19},
  number={2},
  pages={313--330},
  year={1993},
  publisher={MIT Press}
}

@book{francis1964manual,
  title={A Manual of Information to Accompany A Standard Corpus of Present-Day Edited American English, for Use with Digital Computers},
  author={Francis, W. Nelson and Kučera, Henry},
  year={1964},
  publisher={Department of Linguistics, Brown University}
}

@incollection{jelinek1990self,
  title={Self-organized language modeling for speech recognition},
  author={Jelinek, Frederick},
  booktitle={Readings in speech recognition},
  pages={450--506},
  year={1990},
  publisher={Morgan Kaufmann}
}

@inproceedings{kupiec1992robust,
  title={Robust part-of-speech tagging using a hidden Markov model},
  author={Kupiec, Julian},
  booktitle={Proceedings of the conference on Artificial intelligence},
  pages={189--194},
  year={1992}
}

@article{berger1996maximum,
  title={A maximum entropy approach to natural language processing},
  author={Berger, Adam L and Pietra, Vincent J Della and Pietra, Stephen A Della},
  journal={Computational linguistics},
  volume={22},
  number={1},
  pages={39--71},
  year={1996},
  publisher={MIT Press}
}

@misc{beery2018recognitionterraincognita,
      title={Recognition in Terra Incognita}, 
      author={Sara Beery and Grant van Horn and Pietro Perona},
      year={2018},
      eprint={1807.04975},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1807.04975}, 
}

@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{jiang2024manyshotincontextlearningmultimodal,
      title={Many-Shot In-Context Learning in Multimodal Foundation Models}, 
      author={Yixing Jiang and Jeremy Irvin and Ji Hun Wang and Muhammad Ahmed Chaudhry and Jonathan H. Chen and Andrew Y. Ng},
      year={2024},
      eprint={2405.09798},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.09798}, 
}

@misc{chen2024iclevalevaluatingincontextlearning,
      title={ICLEval: Evaluating In-Context Learning Ability of Large Language Models}, 
      author={Wentong Chen and Yankai Lin and ZhenHao Zhou and HongYun Huang and Yantao Jia and Zhao Cao and Ji-Rong Wen},
      year={2024},
      eprint={2406.14955},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.14955}, 
}

@misc{milios2023incontextlearningtextclassification,
      title={In-Context Learning for Text Classification with Many Labels}, 
      author={Aristides Milios and Siva Reddy and Dzmitry Bahdanau},
      year={2023},
      eprint={2309.10954},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.10954}, 
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{dong2024surveyincontextlearning,
      title={A Survey on In-context Learning}, 
      author={Qingxiu Dong and Lei Li and Damai Dai and Ce Zheng and Jingyuan Ma and Rui Li and Heming Xia and Jingjing Xu and Zhiyong Wu and Tianyu Liu and Baobao Chang and Xu Sun and Lei Li and Zhifang Sui},
      year={2024},
      eprint={2301.00234},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2301.00234}, 
}

@inproceedings{shin-etal-2022-effect,
    title = "On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model",
    author = "Shin, Seongjin  and
      Lee, Sang-Woo  and
      Ahn, Hwijeen  and
      Kim, Sungdong  and
      Kim, HyoungSeok  and
      Kim, Boseop  and
      Cho, Kyunghyun  and
      Lee, Gichang  and
      Park, Woomyoung  and
      Ha, Jung-Woo  and
      Sung, Nako",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.380/",
    doi = "10.18653/v1/2022.naacl-main.380",
    pages = "5168--5186",
    abstract = "Many recent studies on large-scale language models have reported successful in-context zero- and few-shot learning ability. However, the in-depth analysis of when in-context learning occurs is still lacking. For example, it is unknown how in-context learning performance changes as the training corpus varies. Here, we investigate the effects of the source and size of the pretraining corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From our in-depth investigation, we introduce the following observations: (1) in-context learning performance heavily depends on the corpus domain source, and the size of the pretraining corpus does not necessarily determine the emergence of in-context learning, (2) in-context learning ability can emerge when a language model is trained on a combination of multiple corpora, even when each corpus does not result in in-context learning on its own, (3) pretraining with a corpus related to a downstream task does not always guarantee the competitive in-context learning performance of the downstream task, especially in the few-shot setting, and (4) the relationship between language modeling (measured in perplexity) and in-context learning does not always correlate: e.g., low perplexity does not always imply high in-context few-shot learning performance."
}

@misc{wei2022emergentabilitieslargelanguage,
      title={Emergent Abilities of Large Language Models}, 
      author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
      year={2022},
      eprint={2206.07682},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2206.07682}, 
}

@misc{min2022rethinkingroledemonstrationsmakes,
      title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?}, 
      author={Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
      year={2022},
      eprint={2202.12837},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2202.12837}, 
}

@misc{liu2021makesgoodincontextexamples,
      title={What Makes Good In-Context Examples for GPT-$3$?}, 
      author={Jiachang Liu and Dinghan Shen and Yizhe Zhang and Bill Dolan and Lawrence Carin and Weizhu Chen},
      year={2021},
      eprint={2101.06804},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2101.06804}, 
}

@misc{lu2022fantasticallyorderedpromptsthem,
      title={Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity}, 
      author={Yao Lu and Max Bartolo and Alastair Moore and Sebastian Riedel and Pontus Stenetorp},
      year={2022},
      eprint={2104.08786},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.08786}, 
}

@misc{si2023measuringinductivebiasesincontext,
      title={Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations}, 
      author={Chenglei Si and Dan Friedman and Nitish Joshi and Shi Feng and Danqi Chen and He He},
      year={2023},
      eprint={2305.13299},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13299}, 
}

@misc{pan2023incontextlearninglearnsincontext,
      title={What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning}, 
      author={Jane Pan and Tianyu Gao and Howard Chen and Danqi Chen},
      year={2023},
      eprint={2305.09731},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.09731}, 
}

@misc{zhang2023doesincontextlearninglearn,
      title={What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization}, 
      author={Yufeng Zhang and Fengzhuo Zhang and Zhuoran Yang and Zhaoran Wang},
      year={2023},
      eprint={2305.19420},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2305.19420}, 
}

@misc{akyürek2023learningalgorithmincontextlearning,
      title={What learning algorithm is in-context learning? Investigations with linear models}, 
      author={Ekin Akyürek and Dale Schuurmans and Jacob Andreas and Tengyu Ma and Denny Zhou},
      year={2023},
      eprint={2211.15661},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.15661}, 
}

@misc{vonoswald2023transformerslearnincontextgradient,
      title={Transformers learn in-context by gradient descent}, 
      author={Johannes von Oswald and Eyvind Niklasson and Ettore Randazzo and João Sacramento and Alexander Mordvintsev and Andrey Zhmoginov and Max Vladymyrov},
      year={2023},
      eprint={2212.07677},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.07677}, 
}

@inproceedings{niven-kao-2019-probing,
    title = "Probing Neural Network Comprehension of Natural Language Arguments",
    author = "Niven, Timothy  and
      Kao, Hung-Yu",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1459/",
    doi = "10.18653/v1/P19-1459",
    pages = "4658--4664",
    abstract = "We are surprised to find that BERT{'}s peak performance of 77{\%} on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work."
}

@inproceedings{Murphy2013TheFL,
  title={The First Level of Super Mario Bros . is Easy with Lexicographic Orderings and Time Travel},
  author={Dr. Tom Murphy},
  year={2013},
  url={https://api.semanticscholar.org/CorpusID:14347703}
}

@misc{zhao2017menlikeshoppingreducing,
      title={Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints}, 
      author={Jieyu Zhao and Tianlu Wang and Mark Yatskar and Vicente Ordonez and Kai-Wei Chang},
      year={2017},
      eprint={1707.09457},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1707.09457}, 
}

@article{Zech2018VariableGP,
  title={Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study},
  author={John R. Zech and Marcus A. Badgeley and Manway Liu and Anthony Beardsworth Costa and Joseph J. Titano and Eric Karl Oermann},
  journal={PLoS Medicine},
  year={2018},
  volume={15},
  url={https://api.semanticscholar.org/CorpusID:49558635}
}

@book{cleverhans,
	title = {Clever Hans (the horse of Mr. Von Osten) a contribution to experimental animal and human psychology  },
	copyright = {Public domain. The BHL considers that this work is no longer under copyright protection.},
	url = {https://www.biodiversitylibrary.org/item/116908},
	note = {https://www.biodiversitylibrary.org/bibliography/56164 --- "Table of references": p. 267-274. --- Supplements i-iv: p. 245-265.},
	publisher = {New York, H. Holt and company, 1911},
	author = {Pfungst, Oskar. and Rahn, Carl Leo.},
	year = {1911},
	pages = {296},
	keywords = {Animal intelligence|Animal training|Horses|Psychology, Comparative|},
}

@misc{steinmann2024navigatingshortcutsspuriouscorrelations,
      title={Navigating Shortcuts, Spurious Correlations, and Confounders: From Origins via Detection to Mitigation}, 
      author={David Steinmann and Felix Divo and Maurice Kraus and Antonia Wüst and Lukas Struppek and Felix Friedrich and Kristian Kersting},
      year={2024},
      eprint={2412.05152},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.05152}, 
}
