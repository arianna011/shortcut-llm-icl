\documentclass[../main.tex]{subfiles}
% !TeX root = ../main.tex
\graphicspath{{\subfix{../assets/}}}
\begin{document}

\chapter*{Introduction}

Both humans and artificial intelligence systems often display a natural tendency to take the easiest path when solving a problem, relying on \textit{shortcuts} that may be effective in specific task instances but fail to generalize across different examples.
For instance, a human student might answer a multiple-choice question by recognizing familiar keywords rather than understanding the underlying concept, or judge the validity of an argument by how confidently it is stated rather than by its logical structure. In a similar way, a medical image classifier may associate the presence of an hospital watermark with a cancer diagnosis, instead of learning the actual visual features of the tumor. Language models, too, often exploit unintended cues: a text classifier might infer that a movie review is positive simply because it contains exclamation marks, or that a news article is political merely because it mentions a politicianâ€™s name.\\

Studies have shown that Large Language Models (LLMs), despite their vast learning capacity, can perform poorly on out-of-distribution inputs \citep{hendrycks2020pretrainedtransformersimproveoutofdistribution} and remain vulnerable to adversarial attacks \citep{wallace2021universaladversarialtriggersattacking}. This lack of robustness is often traced back to \textit{shortcut learning} \citep{Geirhos_2020}, the tendency of models to exploit spurious correlations in the training data rather than learning the intended reasoning process. In Natural Language Processing (NLP), shortcut learning typically manifest as reliance on superficial cues (such as lexical overlap, word order, or stylistic patterns) that correlate with the correct label in standard benchmarks but lead to failures in more realistic settings. Under the In-Context-Learning (ICL) paradigm, this issue becomes even more pronounced: LLMs may attend to shallow patterns in the instruction or examples within the prompt context rather than abstracting the underlying task structure, leading to brittle or biased behavior.\\

Beyond technical robustness, shortcut learning also has important ethical implications. Because training corpora encode social stereotypes and historical imbalances, shortcuts often align with biased associations, like linking occupations to specific genders or ethnic groups \citep{bolukbasi2016mancomputerprogrammerwoman, seshadri2025smallchangeslargeconsequences}. As a result, shortcut-driven behavior can propagate or even amplify unfair outcomes in downstream applications, undermining the accountability and trustworthiness of AI systems in real-world deployments \citep{bender2021danger}.\\

The detection and mitigation of shortcut learning is therefore crucial not only for achieving more robust and generalizable models, but also for ensuring that LLMs behave in ways that are ethically responsible and socially fair. This dual challenge motivates the present work, which investigates Representation Engineering (RepE) \citep{zou2025representationengineeringtopdownapproach} as a novel framework for identifying and mitigating shortcut mechanisms directly within the internal representations of LLMs.
\\

In particular, this study aims to address the following research questions:
\begin{itemize}
    \item Is the concept of "taking a shortcut" explicitely encoded by LLMs in their hidden representations? That is, do models internally know more than they reveal through their generations, similarly to how they encode the truthfulness of statements in cases of hallucination \citep{azaria2023internalstatellmknows}? 
    \item Can a \textit{shortcut reliance} linear direction be identified in the latent representation space using methods such as Principal Component Analysis (PCA)?
    \item Can steering model's predictions at inference time, via targeted amplification or suppression of such direction, lead to more robust and reliable outputs?
\end{itemize}

Answering these questions would clarify whether shortcut learning can be mitigated through a lightweight and interpretable approach such as RepE, which enables the analysis and control of a model's internal representations without requiring any parameters update.\\

Several experiments are conducted to evaluate the effectiveness of the RepE framework when adapted to shortcut detection and mitigation, and to analyze how steering interventions affect model representations, with the goal of assessing both the correctness and the transparency of the proposed method. Given the complexity and novelty of this objective, the chosen case study focuses on a simple yet informative setting: LLMs performing In-Context-Learning on Natural Language Processing tasks such as textual entailment.\\

The content of the thesis is organized as follows. Chapter \ref{chap1} introduces the theoretical background on in-context learning and shortcut learning. Chapter 2 reviews related work on robustness, adversarial vulnerability, and fairness in NLP. Chapter 3 presents the proposed methods for detecting and mitigating shortcut learning in LLMs. Chapter 4 evaluates these approaches empirically, and Chapter 5 discusses their implications for future research and ethical AI deployment. Finally, Chapter 6 concludes the thesis and outlines potential directions for further study.



\subbib{}
\end{document}