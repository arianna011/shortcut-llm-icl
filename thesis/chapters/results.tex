\documentclass[../main.tex]{subfiles}
% !TeX root = ../main.tex
\graphicspath{{\subfix{../assets/}}}
\begin{document}

\chapter{Experimental Evaluation}

Several experiments were conducted to evaluate the effectiveness of the proposed RepE-based framework for shortcut detection and mitigation.  
Section~\ref{sec:setup} describes the experimental setup, while Section~\ref{sec:results} presents and discusses the obtained results. The experiments aim to determine whether latent shortcut directions can be reliably extracted from LLM representations and whether their manipulation through RepControl leads to measurable improvements in model behavior under In-Context Learning (ICL).

\section{Experimental Setup}
\label{sec:setup}

In order to evaluate the framework proposed in Section \ref{sec:method}, two main objectives were considered:

\begin{itemize}
    \item to assess whether shortcut-related mechanisms can be identified within the model’s latent space, thus evaluating \textit{shortcut detection};
    \item to test the effectiveness of the \textit{shortcut mitigation} procedure based on RepControl interventions, determining whether steering the internal representations of LLMs can effectively reduce shortcut-driven behavior under ICL.
\end{itemize} 

Accordingly, this section describes the experimental configuration adopted for both stages, detailing the models, datasets, prompt design and implementation settings used in the analysis.\\

\subsection{Models}

All experiments were carried out using the \textit{Mistral 7B Instruct v0.1} model.\footnote{Available at \url{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1}}  
This choice was motivated by both methodological and practical considerations: Mistral 7B Instruct v0.1 is a decoder-only transformer with open weights, which allows direct access to the model’s internal hidden states, an essential requirement for applying the RepE-based framework. Moreover, despite its moderate size, it demonstrates strong instruction-following and reasoning capabilities, making it representative of modern LLMs while remaining computationally efficient. From a practical perspective, the experiments were conducted on free Google Colab environments, where hardware memory is limited. To enable inference and activation extraction within these constraints, the model was loaded in 4-bit precision using the \textit{bitsandbytes} quantization library. This approach significantly reduced GPU memory usage with minimal degradation in representational fidelity, allowing the experiments to be performed efficiently without altering the qualitative behavior of the model.\\

Mistral 7B Instruct v0.1 follows the transformer-decoder architecture introduced by \textit{Mistral AI}, featuring 32 transformer layers and approximately 7.3 billion parameters.  
Each layer consists of a multi-head self-attention block followed by a feed-forward (MLP) block, both wrapped with \textit{RMSNorm} normalization instead of the standard LayerNorm, improving numerical stability in low-precision inference.  
The attention mechanism uses a \textit{Grouped Query Attention} (GQA) configuration with 8 query heads per group to enhance inference efficiency.  
Positional information is encoded using \textit{rotary positional embeddings} (RoPE), and the model employs \textit{SwiGLU} activation functions in the feed-forward layers.  
The hidden dimension is 4096, with an intermediate feed-forward dimension of 14336 and a context window of up to 8192 tokens \citep{jiang2023mistral7b}.\\

For the purpose of this work, hidden-state representations were extracted after each RMSNorm layer at the output of the transformer blocks.\footnote{Specifically, the hidden representations referred to as $\{H_l\}_{l \in layers}$ correspond to tensors of size $(batch\ size, sequence\ length, hidden\ dimension)$, where the first tensor represents the embedding layer output and each subsequent tensor represents the post-layer activations of the corresponding transformer block.
}
These activations correspond to the contextual embeddings used internally by the model to predict the next token and thus provide the representational substrate targeted by the RepE framework.  
Accessing layerwise activations in this way enables the identification and manipulation of latent directions corresponding to shortcut-related mechanisms.\\

Although the Mistral 7B architecture shares many similarities with LLaMA 2 models (such as the use of rotary positional embeddings and decoder-only transformer blocks), it introduces several design improvements that make it particularly suitable for this study.  
First, Mistral's GQA enables faster computation and lower GPU memory footprint during layerwise extraction and manipulation of activations. Second, as already mentioned, the use of RMSNorm offers better numerical stability when operating under low-precision quantization settings such as the 4-bit configuration adopted here.  
Finally, empirical evaluations have shown that Mistral 7B consistently outperforms LLaMA 2 models of comparable size on most standard benchmarks \citep{jiang2023mistral7b}, suggesting that its latent representations are richer and more semantically structured, an advantageous property for analyzing shortcut mechanisms within the representation space. Overall, Mistral 7B Instruct v0.1 provides an effective balance between accessibility, interpretability, and representational richness for the study of shortcut detection and mitigation.\\

\subsection{Datasets and prompt design}

Most of the conducted experiments are based on the \textit{Textual Entailment Recognition} (TER) task in Natural Language Processing (NLP).  
Also known as \textit{Natural Language Inference} (NLI), TER is an application-independent task that consists in automatically determining whether a directional entailment relationship holds between two text fragments.  
Given a \textit{premise} $P$ and a \textit{hypothesis} $H$, the goal is to decide whether $H$ is \textit{entailed} by $P$, that is, whether $H$ can be considered most likely true if $P$ is true.  
The task can be formulated either as a binary classification problem, where text pairs are labeled as \textit{entailment} or \textit{non-entailment}, or as a three-way classification variant, where they are categorized as \textit{entailment}, \textit{contradiction}, or \textit{neutral}. For example, given the following prompt:
\begin{quote}
\textit{Premise:} A boy is playing in the garden.\\
\textit{Hypothesis:} There is a boy in the garden.\\
\textit{Classification:} ?
\end{quote}
the correct label is \textit{entailment}.  Conversely, for:
\begin{quote}
\textit{Premise:} Luna is barking loudly.\\
\textit{Hypothesis:} Luna is human.\\
\textit{Classification:} ?
\end{quote}
the correct label is \textit{contradiction}.\\

The TER task can be seen as a general evaluation tool for testing the inference capabilities of NLP systems, which all share the need to process and understand natural language, and serves as a unifying framework for reformulating a variety of NLP tasks. For example, \textit{question answering} can be recast as TER by converting each candidate answer into a hypothesis formed from the question-answer pair and ranking them according to the degree of entailment with respect to the given premise. Similarly, in \textit{text summarization} and \textit{machine translation}, candidate summaries or translations can be treated as hypotheses whose quality is estimated by the extent to which they are entailed by the original text \citep{survey2022}.  
Furthermore, TER is particularly well-suited for the present work since it is associated with several well-documented and easily observable shortcut phenomena (e.g. lexical overlap, negation cues, position bias), making it an ideal testbed for studying shortcut detection and mitigation via the proposed RepE-based framework.\\

\noindent \textbf{Training Datasets.} The training datasets referred to as "NLP Shortcut Dataset" in Figure \ref{fig:myrepeframework} were thus taken from the \textit{ShortcutSuite} repository \citep{yuan2024llmsovercomeshortcutlearning}, which provides TER instances augmented with different types of injected shortcuts. In particular, the datasets selected for this work are those derived from the Multi-Genre Natural Language Inference (MNLI) corpus \citep{williams2018broadcoveragechallengecorpussentence}, a 433k-example collection for three-way textual entaiment, spanning ten text genres across both spoken and written English (e.g. telephone speech, government documents, fiction). ShortcutSuite uses a balanced subset of 3,000 premise-hypothesis pairs sampled from the MNLI development set to generate "shortcut-injected" alterations of the original text with the following shortcut types:

\begin{itemize}
    \item \textbf{\textit{Negation}}: Random tautologies containing negation words are appended to the original hypothesis to test whether LLMs are overly sensitive to the mere presence of negation, even when it does not change the underlying logical relationship.  
    For example:
    \begin{quote}
        \small
        \textit{Premise:} Children will enjoy the little steam train that loops around the bay to Le Crotoy in the summer.\\[4pt]
        \textit{Hypothesis (clean):} There is a steam train looping around the bay to Le Crotoy.\\[4pt]
        \textit{Hypothesis (dirty):} There is a steam train looping around the bay to Le Crotoy \textbf{and false is not true}. 
    \end{quote} 
    This corresponds to a form of \textit{Lexicon} shorcut (see Section \ref{sec:types_icl}).
    \vspace{1em}

    \item \textbf{\textit{Position}}: Tautological sentences are inserted at different positions within the hypothesis to evaluate whether the model relies on positional cues rather than semantic content. For instance:
    \begin{quote}
        \small
        \textit{Premise (clean):} Also, the final rule is not intended to have any retroactive effect, and administrative procedures must be exhausted prior to any judicial challenge to the provisions of the rule.\\[4pt]
        \textit{Premise (dirty):}  \textbf{Red is red and red is red and red is red and red is red and red is red.} Also, the final rule is not intended to have any retroactive effect, and administrative procedures must be exhausted prior to any judicial challenge to the provisions of the rule.\\[4pt]
        \textit{Hypothesis:} The final rule isn't meant to have a retroactive effect.
    \end{quote}
    
    \vspace{1em}
    \item \textbf{\textit{Style}}: The original premise is paraphrased into a Bible-like style using the STRAP model, in order to test whether stylistic features influence model predictions independently of semantics. Example:
    \begin{quote}
        \small
        \textit{Premise (clean):} “You and your friends are not welcome here,” said Severn.\\[4pt]
        \textit{Premise (dirty):} \textbf{And Severn said unto him, Thou and thy friends are not welcome here, said he.}\\[4pt]
        \textit{Hypothesis:} Severn said the people were not welcome there.
    \end{quote}
\end{itemize}

These three shortcut types were chosen over the HANS-style shortcuts (\textit{Lexical Overlap}, \textit{Subsequence}, and \textit{Constituent}) included in ShortcutSuite because the RepE framework requires contrastive pairs where the clean and shortcut-injected examples correspond exactly to the same original instance.  
Negation, Position, and Style were the only shortcut sets that explicitly preserve this linkage through shared identifiers across the provided tables, enabling the construction of aligned clean-dirty pairs needed for RepReading.\\

Before being fed to the RepReader, each example was formatted using a simple zero-shot ICL prompt template containing the instruction \texttt{"Decide if the hypothesis is entailed by the premise"}. Below is a complete instance of a contrastive pair of prompts used to extract a latent shorcut direction.\\

\begin{tcolorbox}[colback=white,colframe=blue!40!black!30,boxrule=0.2pt,arc=2pt]
    \texttt{USER: \textcolor{blue!50}{Decide if the hypothesis is entailed by the premise.}\\
     \textcolor{blue!50}{Premise: Children will enjoy the little steam train that loops around the bay to Le Crotoy in the summer.}\\
     \textcolor{blue!50}{Hypothesis: There is a steam train looping around the bay to Le Crotoy.}}\\
    \texttt{ASSISTANT:} 
\end{tcolorbox} 

\begin{tcolorbox}[colback=white,colframe=blue!40!black!30,boxrule=0.2pt,arc=2pt]
    \texttt{USER: \textcolor{blue!50}{Decide if the hypothesis is entailed by the premise.}\\
     \textcolor{blue!50}{Premise: Children will enjoy the little steam train that loops around the bay to Le Crotoy in the summer.}\\
     \textcolor{blue!50}{Hypothesis: There is a steam train looping around the bay to Le Crotoy} \textcolor{red!50}{and false is not true.}} \\
    \texttt{ASSISTANT:} 
\end{tcolorbox} 

\vspace{1em}

\noindent Differently from the original RepE experiments \citep{zou2025representationengineeringtopdownapproach}, in this work the stimuli for the \textit{function} extraction do not rely on model-generated continuations guided by differentiated instructions (e.g. "answer like an honest person" vs "answer like a dishonest person").  
Instead, the task instances themselves serve as stimuli: the model is not required to answer, but only to process the input prompts, thereby generating hidden-state activations that ideally differ between clean and shortcut-augmented versions of the same example.\\


\noindent \textbf{Evaluation Datasets.} The datasets employed for evaluating the proposed methodology (referred to as "NLP Dataset" in Figure \ref{fig:myrepeframework}) are standard NLP benchmarks for the TER task. In particular, in addition to the already cited MNLI, the Recognizing Textual Entailment (RTE) dataset \citep{dagan2006rte} from the GLUE benchmark is used. RTE consists of sentence pairs for entailment vs. not-entailment classification, drawn from the annual RTE challenges held between 2005 and 2011, and built primarily from news and Wikipedia text. While MNLI serves as an \textit{in-distribution} testbed for shortcut detection and mitigation, given that the shortcut training datasets are derived from it, RTE provides an opportunity to test whether the proposed approach generalizes to \textit{naturally occurring} shortcuts in a standard entailment benchmark.\\

For the practical evaluation, the UniBias framework \citep{zhou2024unibiasunveilingmitigatingllm} was used to construct ICL prompts for each test instance.  
In a $k$-shot setting, the ICL context is built as follows:

\begin{itemize}
    \item when $k = 0$, the prompt consists of a dataset-specific task instruction (e.g. for MNLI: \textit{"Given the premise, are we justified in saying the hypothesis? yes, no, or maybe?"}) followed by the test instance;
    \item when $k > 0$, the prompt contains $k$ demonstration examples for each label class, randomly sampled from the dataset training split, followed by the test instance. No explicit task instruction is included in this setting.
\end{itemize}

For illustration, in a 1-shot setting, an actual RTE prompt seen by the model at test time may be:

\begin{quote}
\small
\textit{Premise:} Former Prime Minister Rafik Hariri, also a prominent anti-Syria political figure, was killed in a suicide bombing in February last year, which led to rising anti-Syrian waves and the withdrawal of Syrian troops from Lebanon.\\
\textit{Hypothesis:} Syrian troops have been withdrawn from Lebanon after the murder of Rafik Hariri.\\
\textit{Answer: yes}

\vspace{0.4em}

\textit{Premise:} The organizers of the 15th International AIDS Conference, scheduled for next month in Bangkok, Thailand, on Saturday responded to press reports that a prominent hotel in Bangkok discriminated against HIV-positive visitors attending a different conference this month.\\
\textit{Hypothesis:} HIV-positive visitors take part in the 15th International AIDS Conference.\\
\textit{Answer: no}

\vspace{0.4em}

\textit{Premise:} Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44, according to the Christopher Reeve Foundation.\\
\textit{Hypothesis:} Christopher Reeve had an accident.\\
\textit{Answer:}
\end{quote}

To investigate the generalizability of the proposed shortcut mitigation framework beyond entailment tasks, additional NLP benchmarks were considered.  
Specifically, the following datasets were employed:

\begin{itemize}
    \item \textbf{COPA} \citep{gordon-etal-2012-semeval}: a commonsense causal reasoning task in which the model chooses between two plausible alternatives to complete a causal relation with a given premise;
    \item \textbf{CR} \citep{turney2002thumbsthumbsdownsemantic}: a binary sentiment classification dataset consisting of customer reviews labeled as positive or negative;
    \item \textbf{SST2} \citep{socher-etal-2013-recursive}: the Stanford Sentiment Treebank binary classification task on movie reviews;
    \item \textbf{WiC} \citep{pilehvar-camacho-collados-2019-wic}: the Word-in-Context benchmark, which tests word sense disambiguation by asking whether a target word has the same meaning in two different sentences;
    \item \textbf{ARC} \citep{clark2018thinksolvedquestionanswering}: a multiple-choice question answering dataset for grade-school science questions;
    \item \textbf{MMLU} \citep{hendrycks2021measuringmassivemultitasklanguage}: the Massive Multitask Language Understanding benchmark, a large-scale multiple-choice collection spanning diverse subjects such as mathematics, history, law and computer science.
\end{itemize}

Evaluating the framework on tasks beyond entailment is also valuable for assessing whether shortcut-related representations learned by LLMs exhibit any degree of cross-task consistency.  
If similar shortcut directions emerge across heterogeneous datasets, ranging from sentiment analysis to causal reasoning or word-sense disambiguation, this would suggest the presence of shared latent mechanisms through which LLMs internalize and exploit superficial cues.  
Conversely, a lack of transferability would indicate that shortcut reliance is highly task-specific, reinforcing the need for targeted analyses.  
Thus, examining multiple NLP tasks provides insights into whether shortcut representations capture generalizable patterns or remain confined to the TER setting from which they were extracted.\\

Given the exploratory nature of the experiments and the limited computational resources available, the test set sizes were kept modest.  
Table \ref{tab:testsets} reports the test sets used in this work, along with their source splits and number of examples.

\begin{table}[ht]
\centering
\begin{tabular}{l c c}
\toprule
\textbf{Name} & \textbf{Source} & \textbf{Size} \\
\midrule
MNLI & subset of MNLI "validation matched" split & 500 \\
RTE & RTE validation split & 277 \\
COPA & COPA validation split & 100 \\
CR & CR test split & 376 \\
SST2 & SST2 validation split & 872 \\
WIC & WiC validation split & 638 \\
ARC & ARC test split & 1170 \\
MMLU & subset of MMLU test split & 2000 \\
\bottomrule
\end{tabular}
\vspace{6pt}
\caption{Evaluation test sets used in this work}
\label{tab:testsets}
\end{table}



\section{Results and Analysis}
\label{sec:results}

This section presents the results obtained from the experiments evaluating the RepE-based approach to shortcut detection and mitigation.  
Shortcut detection (Section \ref{sec:detres}) is assessed qualitatively on selected test instances, while shortcut mitigation (Section \ref{sec:mitres}) is evaluated systematically across multiple hyperparameter configurations on different test sets (Table \ref{tab:testsets}).
To support reliable tracking and reproducibility, all experiments were logged using the Weights \& Biases (W\&B) platform, which recorded both input configurations and output metrics.  
In addition, the full sets of prompts and activation files used in each run were stored as artifacts, ensuring that every experiment can be reproduced precisely.

\subsection{Shortcut Detection Results}
\label{sec:detres}

To qualitatively assess the effectiveness of the RepE-based framework for shortcut detection, the following experiment was conducted. Given a RepReader $R$ trained on a random subset of the negation shortcut dataset\footnote{The negation shortcut type was selected because it is easier to visualize, being tied to explicit lexical markers such as "not", and because its effect on the model’s predictions is straightforward: an entailment relation is often incorrectly interpreted as non-entailment.} from the ShortcutSuite and two contrastive test statements $s1$ and $s2$ (exhibiting or non-exhibiting an explicit shortcut cue), scores $A_{l,t}$ are computed to measure, at different layers $l$ and token positions $t$, the degree to which the model’s internal representations of $s1$ and $s2$ align with the shortcut direction identified by $R$. The scores computation proceeds as follows:

\begin{itemize}
    \item \textbf{Encoding of the input statements.} The sentences $s1$ and $s2$ are fed to the frozen target LLM to obtain hidden representations $h_{l}$ for each layer $l$ and token position $t$. Optionally, the model's generated continuations for $s1$ and $s2$ are appended to the input before extracting the hidden representations, allowing shortcut effects to surface in both the prompt and the generation.
    
    \item \textbf{Projection onto the shortcut direction.} For each layer $l$ and token position $t$, the hidden representations $h_{l, t}$ is projected onto the shortcut direction $V_l$ inferred by the RepReader $R$ for that layer. The projection is then multiplied by the corresponding sign $S_l$: \[
    A_{l,t} = \langle h_{l,t}, V_l \rangle \cdot S_l
    \]

    \item \textbf{Averaged projections.} Given a selected set of hidden layers $L$, the scores $A_{l,t}$ are averaged across all laters in $L$ to obtain a mean shortcut-alignmet score for each token position 
    \[
    A^{\text{mean}}_{t} = \frac{1}{|H|} \sum_{l \in H} A_{l,t}
    \]
    This aggregation highlights persistent shortcut activation patterns across layers rather than isolated spikes.
\end{itemize}

\vspace{1em}

Scores $A_{l,t}$ can be visualized as an heatmap across layers and token positions, a representation referred to as a \textit{LAT scan} in the original RepE paper. In the honesty experiments, the authors observed clearly distinguishable scans for honest versus dishonest generations, with the former showing a substantially stronger correlation with the learned "honesty direction". In the shortcut detection setting of the present work, the contrast between the scans of the two test statements is less pronounced. Nonetheless, subtle differences in the LAT scans can still be observed, indicating that the shortcut direction captured by the RepReader is activated to a different extent across the two inputs.\\

\noindent Figures \ref{fig:clean_scan} and \ref{fig:dirty_scan} show the LAT scans corresponding to $s1$ =

\begin{quote}
    "[INST] Is the hypothesis entailed by the premise? yes or no. [/INST] Premise: Managing better requires that agencies have, and rely upon, sound financial and program information. Hypothesis: Agencies need sound financial and program information for good management."
\end{quote}

and $s2$ = \begin{quote}
    "[INST] Is the hypothesis entailed by the premise? yes or no. [/INST] Premise: Managing better requires that agencies have, and rely upon, sound financial and program information. Hypothesis: Agencies need sound financial and program information for good management \textbf{and green is not red}."
\end{quote} 
each augmented with the continuation generated by the model.


\begin{figure}[h]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/clean_LAC_management.png}
        \caption{LAT scan of a statement not exhibiting an explicit shorcut cue (\textit{clean} scan)}
        \label{fig:clean_scan}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/dirty_LAC_management.png}
        \caption{LAT scan of a statement exhibiting an explicit shorcut cue (\textit{dirty} scan)}
        \label{fig:dirty_scan}
    \end{minipage}

\end{figure}

\noindent In the deepest hidden layers (approximately layers 
-10 to -1), both scans display a strong correlation with the shortcut direction (dark red cells). This suggests that these late layers are intrinsically more sensitive to the shortcut feature, independently of the specific input. Nevertheless, some meaningful differences between the two scans can be observed. For token positions 40-45, the clean scan exhibits a negative correlation with the shortcut direction, whereas the dirty scan shows an extended red band, indicating shortcut activation even in earlier layers. Furthermore, at the final token position (which is expected to summarize the representation of the entire input sequence) the clean scan shows shortcut correlation only in the last layer, while the dirty scan displays it across multiple layers.\\

To gain further insight into the semantic direction captured by the RepReader, the averaged scores $A^{\text{mean}}_{t}$ can be plotted over the corresponding tokens to identify which words correlate most strongly with the learned shortcut direction. Figure \ref{fig:detection} presents the detection results for the $s2$ statement together with its generated continuation, truncated after a fixed number of tokens. Darker green shading indicates stronger alignment with the shortcut direction. Most highlighted tokens appear as isolated occurrences and correspond to common high-frequency words (such as "that", "have", "and") which are likely attributable to noise. Nevertheless, portions of the shortcut cue "and green is not red" also exhibit clear activation, suggesting that the RepReader partially identifies and responds to the shortcut structure present in the input.\\

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{assets/detection_management_dirty.png}
    \caption{Visualization of token-level projection scores onto the shortcut direction, averaged across selected hidden layers}
    \label{fig:detection}
\end{figure}

The outcomes of the shortcut detection experiments offer insight into the effectiveness and interpretability of the RepE framework. The presence of consistent shortcut-sensitive activations across multiple layers suggests that such directions are not confined to a small subset of the network. Instead, they appear to be distributed across depth, emerging cumulatively and becoming more pronounced in the later layers. This supports the view that shortcuts in LLMs are not isolated artifacts but reflect broader representational tendencies embedded in the model’s overall geometry. These observations indicate that RepE is a promising, though not yet fully disentangled, interpretability tool for detecting shortcut reliance. Its results highlight both the potential of directional approaches and the inherent complexity of probing model representations through linear interpretive methods.

\subsection{Shortcut Mitigation Results}
\label{sec:mitres}

To evaluate the effectiveness of the proposed RepE-based framework described in Section \ref{sec:method} for mitigating shortcut learning, the Mistral 7B Instruct v0.1 model was evaluated on ICL-based classification tasks both in its base version and under RepControl interventions. Model performance was assessed using standard classification metrics, including accuracy and F1-score.\\

A grid search over multiple hyperparameter configurations was conducted to identify effective intervention settings and to analyze the sensitivity of the mitigation performance to different control parameters. Table \ref{tab:hyperparams} summarizes the hyperparameters and the corresponding values explored in the experiments.

\begin{table}[ht]
\centering
\begin{tabular}{p{3.5cm} p{5.5cm} p{3.5cm}}
\toprule
\textbf{Hyperparameter} & \textbf{Description} & \textbf{Values Tested} \\
\midrule
RepReader Type & Method used to extract the shortcut direction from hidden representations & PCA, ClusterMean \\

Control Operator & RepControl operation applied to hidden states & Linear Combination, Piecewise-linear, Projection \\

Intervention Layers & Range of transformer layers where control is applied & \{from -5 to -17\}, \{from -8 to -22\}, \{from -10 to -29\} \\

Representation Token & Token position used for intervention & Last input token \\

Intervention Strength ($\alpha$) & Scaling coefficient controlling the magnitude of the intervention & $\{-2.0,\ -1.5,\ -1.0$ $-0.5,\ -0.3,\ 1.0,$ $\ 1.5\}$ \\

Shot Setting ($k$) & Number of ICL demonstrations per class & $k \in \{0,1\}$ \\

Training shortcut & Type of shortcut cue present in the dataset used to extract the latent direction & Negation, Position, Style \\

Training dataset size & Number of prompt pairs used to extract clean and dirty hidden representations & $\{64,\ 128,\ 256,\ 512\}$\\

Training prompt selection & Criterion used to select prompts from the ShortcutSuite training set  & Random, model-failure-only\\

\bottomrule
\end{tabular}
\vspace{6pt}
\caption{Hyperparameter tested for shortcut mitigation experiments.}
\label{tab:hyperparams}
\end{table}



\begin{table}[h!]
\centering
\begin{tabular}{l *{4}{cc}}
\toprule
    & \multicolumn{2}{c}{\textbf{Baseline}}
    & \multicolumn{2}{c}{\textbf{PCA+LinComb}}
    & \multicolumn{2}{c}{\textbf{PCA+PieceLin}}
    & \multicolumn{2}{c}{\textbf{PCA+Proj}} \\
\midrule
    & Acc & F1
    & Acc & F1
    & Acc & F1
    & Acc & F1 \\
\cmidrule(lr){2-3}      % under Baseline
\cmidrule(lr){4-5}      % under PCA + LinComb
\cmidrule(lr){6-7}      % under PCA + PieceLin
\cmidrule(lr){8-9}      % under PCA + Proj
\textbf{RTE}
    & 0.787 & 0.784 & 0.805 & 0.800 & \textbf{0.834} & \textbf{0.832} & 0.823 & 0.822 \\
\textbf{MNLI}
    & 0.598 & 0.488 & 0.622 & 0.503 & 0.000 & 0.000 & 0.000 & 0.000 \\
\textbf{COPA}
    & 0.000 & 0.000 & \textbf{0.880} & \textbf{0.879} & 0.870 & 0.867 & \textbf{0.880} & \textbf{0.879} \\
\textbf{CR}
    & 0.000 & 0.000 & \textbf{0.939} & \textbf{0.933} & 0.931 & 0.924 & 0.926 & 0.918 \\
\textbf{SST2}
    & 0.000 & 0.000 & 0.953 & 0.953 & 0.952 & 0.952 & 0.950 & 0.949 \\
\textbf{WIC}
    & 0.000 & 0.000 & 0.525 & 0.450 & 0.547 & 0.533 & 0.508 & 0.406 \\
\textbf{ARC}
    & 0.000 & 0.000 & 0.685 & 0.684 & 0.682 & 0.680 & 0.677 & 0.676 \\
\textbf{MMLU}
    & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\
\bottomrule
\end{tabular}
\vspace{6pt}
\caption{Best accuracy and F1 scores obtained for the base Mistral-7B-Instruct model and three RepControl configurations using the PCA representation reader with the following operators: linear combination (LinComb), piecewise-linear transformation (PieceLin) and projection (Proj).}
\end{table}

% instance specific repcontrol
% shortcut aggregation experiment

\subsection{Qualitative Analysis}

\subbib{}
\end{document}