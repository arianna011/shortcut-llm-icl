\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../assets/}}}
\begin{document}

\begin{abstract}

\noindent While Large Language Models (LLMs) are extremely powerful and versatile tools, their robustness and interpretability are often undermined by shortcut learning, a phenomenon in which models rely on spurious correlations between inputs and outputs rather than on genuine semantic or causal reasoning. Such behavior can lead to brittle generalization, misleading predictions and the amplification of undesirable biases.\\

\noindent This thesis investigates the detection and mitigation of shortcut learning in LLMs under an In-Context Learning (ICL) setting. The ICL paradigm introduces additional opportunities for shortcut exploitation, as models may leverage superficial cues in demonstration examples such as lexical overlap, positional patterns or stylistic regularities, instead of internalizing the intended task structure. To address this challenge, this work adapts the recently proposed Representation Engineering (RepE) framework to shortcut learning. The proposed approach operates directly on the internal representations of a frozen LLM, extracting latent directions associated with shortcut reliance through contrastive analysis of clean and shortcut-augmented inputs, and manipulating these directions at inference time to suppress shortcut-driven behavior without updating model parameters.\\

\noindent An extensive empirical evaluation is conducted using the Mistral 7B Instruct model under ICL across multiple NLP benchmarks, including textual entailment, sentiment analysis, commonsense reasoning, word sense disambiguation and multi-domain question answering. The results show that shortcut-aligned directions can be reliably identified in the modelâ€™s representation space and that their targeted manipulation leads to consistent performance improvements across tasks, even when the shortcut direction is extracted from a single task domain.\\

\noindent Overall, this work demonstrates that shortcut learning in LLMs can be addressed through lightweight, interpretable and training-free interventions at the representation level, providing new insights into the internal mechanisms underlying shortcut reliance and highlighting the potential of representation-based control for improving model robustness and generalization.
\end{abstract}

\subbib{}
\end{document}