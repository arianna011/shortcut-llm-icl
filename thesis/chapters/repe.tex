\documentclass[../main.tex]{subfiles}
% !TeX root = ../main.tex
\graphicspath{{\subfix{../assets/}}}
\begin{document}

\chapter{Method}

% method outline + chapter contents

\section{Representation Engineering}

Representation Engineering (RepE) \citep{zou2025representationengineeringtopdownapproach} is a recent approach to the interpretability analysis of Large Language Models (LLMs). It aims to enhance human understanding of their internal behaviors, thereby enabling a more informed and effective use of such systems, while mitigating the risks posed by hidden erroneous mechanisms, which represent a critical hazard given the growing pervasiveness of AI across society.\\

RepE finds theoritical grounding in the \textit{Hopfieldian} view of cognitive neuroscience, which interprets cognition as computation over representational spaces emerging from the activity of neural populations, abstracting away the specific connections betweeen individual neurons. This perspective focuses on high-level cognitive phenomena, contrasting with the \textit{Sherringtonian} view, which centers on single neurons and node-to-node interactions \citep{cognitivescience}. While the field of Mechanistic Interpretability\footnote{A branch of AI research that seeks to "reverse-engineer" neural networks by explaining their outputs in terms of circuits and algorithms} adopts a Sherringtonian-like, bottom-up approach by studying the relations between hidden units (e.g. attention heads or MLP neurons) and model behaviors, RepE proposes a complementary top-down interpretation of LLMs. It aims to link emergent phenomena within models to their hidden representations, which have been shown to become increasingly semantically structured across layers \citep{tenney2019bertrediscoversclassicalnlp}. This may overcome the limits of bottom-up approaches in capturing complex and distributed effects in large networks, emphasizing how LLMs encode human concepts in latent spaces rather than focusing on their specific neural architecture.\\

In simple terms, RepE operates by extracting an LLM's internal representations of selected concepts (e.g. emotion, utility, honesty) from carefully crafted input prompts using so-called \textit{RepReading} methods, and subsequently by employing these representations to control and manipulate the frozen modelâ€™s generations at inference time through \textit{RepControl} approaches (Figure \ref{fig:repeoverview}).

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{assets/RepE_overview_no_title.png}
    \caption{Overview of the main pipeline of the Representation Engineering (RepE) framework}
    \label{fig:repeoverview}
\end{figure}

\subsection{RepReading}
\label{subsec:repreading}

The goal of \textit{RepReading} methods is to extract the neural activity associated with a given high-level human concept from a target LLM, in order to enhance the understanding of the model's internal representations and enable subsequent monitoring and control of its generations.\\

More formally, the authors of RepE \citep{zou2025representationengineeringtopdownapproach} distinguish between \textit{concepts} and \textit{functions}. The former refer to general notions like morality or truthfulness, whereas the latter describe behavioral processes enacted by the LLM through its outputs, such as lying or power-seeking. To elicit both concept and function representations, they propose a baseline method called \textit{Linear Artificial Tomography} (LAT), which consists of three main steps:
\begin{enumerate}
    \item Defining an appropriate task template;
    \item Collecting the corresponding neural activity from the LLM;
    \item Extracting the desired concept or function representation using a linear model.\\
\end{enumerate}

\noindent\textbf{Step 1.} Designing a suitable prompt template to feed the target LLM in order to stimulate the neural activity to be extracted is a crucial step in the RepReading pipeline. The model input must both isolate and fully capture the desired concept or function within the corresponding produced representations. To extract the model's understanding of a given concept $c$, the proposed template $T_c$ for decoder-based LLMs is: \\

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,boxrule=0.2pt,arc=2pt]
\texttt{Consider the amount of \textcolor{red!50}{<concept>} in the following: }\\[2pt]
\texttt{\textcolor{blue!50}{<stimulus>}}\\
\texttt{The amount of \textcolor{red!50}{<concept>} is}
\end{tcolorbox}

\noindent Here, the \textit{stimulus} consists in unlabeled or self-generated statements that vary in the intensity of the concept's presence. This setup allows the extraction of the model's declarative knowledge about specific semantic areas. For instance, if the concept $c$ corresponds to an emotion (e.g. "anger"), the model is stimulated to estimate its presence in a given scenario, thereby activating specific patterns in its hidden representations.\\

To elicit the procedural knowledge related to a function $f$, instead, two contrastive task-related prompts are required: an \textit{experimental} prompt that induces the model to execute the function $f$ and a \textit{reference} prompt that does not. They are used to generate, respectively, the templates $T^+_f$ and $T^-_f$, which share the following structure:\\

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,boxrule=0.2pt,arc=2pt]
\texttt{<USER TAG> \textcolor{blue!50}{<instruction>} \textcolor{red!50}{<experimental/reference prompt>} }\\[2pt]
\texttt{<ASSISTANT TAG> \textcolor{blue!50}{<output>}}
\end{tcolorbox}

\noindent In this case, the \textit{stimulus} is composed of the \textit{<instruction>} and \textit{<output>} fields, which can be derived from instruction-tuning datasets and do not contain explicit labels related to the target function, thus making the procedure unsupervised. For example, if the function $f$ corresponds to "power-seeking", the two contrastive prompts could be:\\

\begin{tcolorbox}[colback=white,colframe=green!50,boxrule=0.2pt,arc=2pt]
\texttt{USER: \textcolor{blue!50}{Pretend you're an} \textcolor{red!50}{ambitious, power-seeking} \textcolor{blue!50}{person. Tell me what you want to do.}}\\[2pt]
\texttt{ASSISTANT: \textcolor{blue!50}{<output>}}
\end{tcolorbox} and \begin{tcolorbox} [colback=white,colframe=green!50,boxrule=0.2pt,arc=2pt] \texttt{USER: \textcolor{blue!50}{Pretend you're a} \textcolor{red!50}{docile, power-aversive} \textcolor{blue!50}{person. Tell me what you want to do.}}\\[2pt]
\texttt{ASSISTANT: \textcolor{blue!50}{<output>}}
\end{tcolorbox}

\vspace{1em} 
\noindent\textbf{Step 2.} After crafting the input for the transformer-based LLM whose knowledge is to be analyzed, hidden representations can be extracted from different layers and token positions. For decoder models, the representations corresponding to a concept $c$ are taken from the last token preceding the model's prediction (i.e. the final token in the template $T_c$). According to the Next Token Prediction objective used during LLM pretraining, this position is expected to contain the richest concept-specific signal, as it marks the transition point where the model shifts from understanding the semantic query (e.g. "The amount of honesty is") to producing its estimation.\\

The set of activations $A_c$ collected for a concept $c$ from a decoder model $M$, given a set of stimuli $S$, can be formalized as:

\begin{equation}
    A_c = \{\ Rep(M,T_c(s_i))[-1] \mid s_i \in S\ \}
\end{equation}
\noindent where $Rep$ denotes a function that takes a model and an input, returning the model's representations from all layers and token positions ($-1$ indicates the last token in the input sequence).\\

To gather the activations corresponding to a function $f$, instead, all tokens from the model's response to the input stimulus are considered, since the LLM should exhibit the function's behavior throughout its generated sequence (i.e. the tokens in the \textit{<output>} field of the $T_f$ template). Thus, the representations $A_f^\pm$ extracted from the experimental and reference prompts can be defined as:

\begin{equation}
    A_f^\pm = \{ \ Rep(M, T_f^\pm(q_i, a_i^k))[-1] \mid (q_i,a_i) \in S, 0 < k \le |a_i|\ \}
\end{equation}

\noindent where $(q_i,a_i)$ denotes a pair consisting of a question and the corresponding answer (the \textit{<instruction>} and \textit{<output>} fields in $T_f$) and $a_i^k$ represents the partial answer truncated after token $k$.\\

\noindent\textbf{Step 3.} Finally, a latent direction identifying the target concept or function is estimated from the collected neural activity of the LLM by leveraging a linear model. Both supervised approaches (e.g. linear probing, difference between cluster means) and unsupervised ones (e.g. PCA, K-means) can be employed, although the original RepE paper primarily adopts Principal Component Analysis (PCA).\\

In particular, to construct the input for PCA:
\begin{itemize}
    \item the set of input stimuli $S$ is divided into pairs $(s_i, s_{i+1})$, where each pair ideally differs only in the presence or intensity of the target concept or function;
    \item hidden states at the selected token position are collected for each stimulus in every pair, generating a collection $H = [\{H(s_0), H(s_1)\},\ \{H(s_2), H(s_3)\}, ...]$;
    \item the relative difference between the hidden states within each pair is computed and normalized, yielding the PCA input $D = [normalize(H(s_0) - H(s_1)),\ normalize(H(s_2) - H(s_3)), ...]$.
\end{itemize}

\vspace{1em} 
\noindent Specifically, PCA is computed on $\{A_c^{(i)} - A_c^{(j)} \}$ for concepts and on $\{(-1)^i (A_f^{+(i)} - A_f^{-(i)})\}$ for functions, where the signs of the experimental and reference representations are changed across pairs.\\

\noindent The first principal component derived from PCA is referred to as the \textit{Reading Vector} $v$ and represents a latent direction for each hidden layer. In practice, $v$ may have a different sign for each layer, indicating the direction that maximizes the activation of the elicited concept or function. This sign can be estimated by using labeled training pairs: by projecting the corresponding hidden states onto the direction $v$, the alignment of the "positive" example in the pair (i.e. the one expressing the target concept or function more strongly) with either the highest or lowest projection value determines the correct orientation.\\

To use the Reading Vector in downstream analysis on a test set $S_{\text{test}}$, hidden states $H_{\text{test}}$ are extracted and projected onto $v$ via dot product, obtaining scalar scores that are intended to quantify the amount of the target concept or function encoded in those representations.

\subsection{RepControl}

While \textit{RepReading} provides a means to extract a model's internal representations of specific concepts, \textit{RepControl} enables the manipulation of these representations at inference time, allowing to amplify or suppress the neural activity related to a given concept or function, without updating any of the model's parameters. In the case of safety-relevant concepts, such as "bias" or "toxicity", RepControl could help mitigate the risks associated with harmful or undesired LLM generations.\\

The authors of RepE \citep{zou2025representationengineeringtopdownapproach} introduce several baseline transformations of hidden representations, based on different types of \textit{controllers}, which represent the operands of the transformations, and \textit{operators}, which specify the mathematical operations that can be applied to them.\\

\noindent\textbf{Controllers.} Controllers are vectors or matrices that interact with the target model's hidden states to modify its behavior. The controllers proposed in the original RepE paper are:

\begin{itemize}
    \item \textit{Reading Vector}: the reading vector obtained through a RepReading method such as LAT (presented in Section \ref{subsec:repreading}) captures the model's internal representation of a concept or function and can be therefore used to control the intensity of their presence in the model's activations. However, since it is precomputed before the actual intervention at inference time, it constitutes a stimulus-indepedent approach, meaning that all representations are perturbed towards the same direction regardless of the input;
    \item \textit{Contrast Vector}: an alternative approach consists in running a pair of contrastive prompts through the target LLM at intervention time to produce two representantions and take their difference, yielding a stimulus-dependent contrast vector. While method can produce more accurate control, it introduces computational-overhead due to the additional forward passes required at inference time;
    \item \textit{Low-Rank Representation Adaptation (LoRRA)}: a more complex approach consists in attaching low-rank adapters to the target model's attention weights and tuning them by minimizing a reconstruction loss between the current and target representations, computed as $r^t_l = R(M, l, x_i) + \alpha v_l^c + \beta v_l^r$ for each layer $l$, where $v_l^c = R(M, l, x_i^+) - R(M, l, x_i^-)$ is a contrast vector obtained from contrastive inputs $x_i^+$ and $x_i^-$, and $v_l^r$ is an optional reading vector. In this case, the operands correspond to the tuned low-rank attention weight matrices.
\end{itemize}

\vspace{1em} 
\noindent\textbf{Operators.} Depending on the specific control objective, different operations can be performed to transform the original model representations $R$ into controlled representations $R'$ using a generic controller $v$:

\begin{itemize}
    \item \textit{Linear Combination}: \begin{equation} R' = R \pm v \end{equation} In this case, the behavior encoded by $v$ can be either stimulated or suppressed;
    \item \textit{Piece-wise Operation}: \begin{equation}R' = R + sign(R^Tv)v \end{equation} Here, the neural activity along the direction of $v$ is conditionally amplified based on its alignment with the representation;
    \item \textit{Projection}: \begin{equation}R' = R - \frac{R^T v}{\|v\|^2} v\end{equation} In this case, the component of the representation aligning with $v$ is removed by projecting it out.
\end{itemize}


\vspace{1em} 
A simple way to evaluate the effectiveness of RepControl is to perform manipulation experiments that test for a causal relationship between representation modification and changes in the model's predictions. However, further interpretability analysis may be required to rule out the possibility that positive outcomes result from chance correlations or unintended side effects.

\subsection{Example Application: Honesty in LLMs}

The original RepE paper \citep{zou2025representationengineeringtopdownapproach} presents several compelling examples illustrating the application of the proposed framework. For instance, it provides a detailed analysis on experiments conducted with \textit{RepReading} and \textit{RepControl} on concepts and functions related to \textit{honesty} in LLMs.\\

In the context of AI models, honesty refers to the consistency between a model's outputs and its internal beliefs, which can be investigated through interpretability techniques. When a model produces false statements, this may result either from a lack of capacity (i.e. the model genuinely believes something that is factually incorrect) or from \textit{lying} behavior, which RepE experiments sought to detect.\\

\noindent First, the authors demonstrated that LLMs possess an internal and consistent concept of truthfulness that is not always aligned with their generations. By applying the LAT technique, they extracted a "truthfulness" direction from datasets of true-false statements and showed that it is able to achieve superior performance on standard question-answering benchmarks compared to few-shot prompting. Moreover, since the direction was derived from diverse data sources, its strong performance suggests a high degree of generalizability.\\

\noindent By comparing LLM predictions based on the truthfulness LAT direction and zero-shot model performance, the authors concluded that large models possess reliable internal representations of truth but often produce outputs that deviate from them, exhibiting a form of \textit{dishonest} behavior. A simple lie detector was constructed by summing the negated honesty scores across selected hidden layers at each token position of a given statement. It successfully identified various forms of falsehoods, hallucination and misleading information. The LAT reading vector was also employed as a classifier of true and false statements on an held-out in-distribution test set, achieving an accuracy of approximately 90\%. A marked contrast was observed in the neural activations associated to honest versus dishonest generations (e.g. "The president of the United states in 2018 is Donald Trump" versus "The president of the United States in 2030 is Elizabeth Warren").\\

\noindent Furthermore, by adding the honesty vector to the LLM activations via RepControl techniques, the authors succesfully steered model responses toward more or less honest answers, demonstrating a strong counterfactual effect and improving the baseline zero-shot performance on TruthfulQA MC1 by up to 19\%.\\

Other examples presented in the paper included experiments on ethically critical themes such as immorality and power-seeking tendencies of LLMs, as well as studies on emotions, bias and fairness, memorization, knowledge and model editing. This broad range of application areas highlights the vast potential of the RepE framework and served as one of the motivations for its adoption in the present work on shorcut learning detection and mitigation.

\section{Shortcut detection and mitigation with RepE}

\begin{sidewaysfigure}
    \centering
    \includegraphics[width=1.0\textheight]{assets/RepE_framework_no_title_cropped.pdf}
    \caption{Overview of the Representation Engineering (RepE) framework adapted to shortcut mitigation}
    \label{fig:myrepeframework}
\end{sidewaysfigure}



\end{document}