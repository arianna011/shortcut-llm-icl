\documentclass[../main.tex]{subfiles}
% !TeX root = ../main.tex
\graphicspath{{\subfix{../assets/}}}
\begin{document}

\chapter{Method}

The present work aims to address the problem of \textit{shorcut learning} in Large Language Models (LLMs) under In-Context Learning (ICL) by adapting the recently proposed Representation Engineering (RepE) framework \citep{zou2025representationengineeringtopdownapproach} to this new context, with the goal of developing a training-free and interpretable approach to shortcut detection and mitigation. The original framework is described in Section \ref{sec:repe}, followed by its integration into the proposed shortcut mitigation framework discussed in Section \ref{sec:method}.

\section{Representation Engineering}
\label{sec:repe}

Representation Engineering (RepE) \citep{zou2025representationengineeringtopdownapproach} is a recent approach to the interpretability analysis of LLMs. It aims to enhance human understanding of their internal behaviors, thereby enabling a more informed and effective use of such systems, while mitigating the risks posed by hidden erroneous mechanisms, which represent a critical hazard given the growing pervasiveness of AI across society.\\

RepE finds theoritical grounding in the \textit{Hopfieldian} view of cognitive neuroscience, which interprets cognition as computation over representational spaces emerging from the activity of neural populations, abstracting away the specific connections betweeen individual neurons. This perspective focuses on high-level cognitive phenomena, contrasting with the \textit{Sherringtonian} view, which centers on single neurons and node-to-node interactions \citep{cognitivescience}. While the field of Mechanistic Interpretability\footnote{A branch of AI research that seeks to "reverse-engineer" neural networks by explaining their outputs in terms of circuits and algorithms} adopts a Sherringtonian-like, bottom-up approach by studying the relations between hidden units (e.g. attention heads or MLP neurons) and model behaviors, RepE proposes a complementary top-down interpretation of LLMs. It aims to link emergent phenomena within models to their hidden representations, which have been shown to become increasingly semantically structured across layers \citep{tenney2019bertrediscoversclassicalnlp}. This may overcome the limits of bottom-up approaches in capturing complex and distributed effects in large networks, emphasizing how LLMs encode human concepts in latent spaces rather than focusing on their specific neural architecture.\\

In simple terms, RepE operates by extracting an LLM's internal representations of selected concepts (e.g. emotion, utility, honesty) from carefully crafted input prompts using so-called \textit{RepReading} methods, and subsequently by employing these representations to control and manipulate the frozen model’s generations at inference time through \textit{RepControl} approaches (Figure \ref{fig:repeoverview}).

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{assets/RepE_overview_no_title.png}
    \caption{Overview of the main pipeline of the Representation Engineering (RepE) framework}
    \label{fig:repeoverview}
\end{figure}

\subsection{RepReading}
\label{subsec:repreading}

The goal of \textit{RepReading} methods is to extract the neural activity associated with a given high-level human concept from a target LLM, in order to enhance the understanding of the model's internal representations and enable subsequent monitoring and control of its generations.\\

More formally, the authors of RepE \citep{zou2025representationengineeringtopdownapproach} distinguish between \textit{concepts} and \textit{functions}. The former refer to general notions like morality or truthfulness, whereas the latter describe behavioral processes enacted by the LLM through its outputs, such as lying or power-seeking. To elicit both concept and function representations, they propose a baseline method called \textit{Linear Artificial Tomography} (LAT), which consists of three main steps:
\begin{enumerate}
    \item Defining an appropriate task template;
    \item Collecting the corresponding neural activity from the LLM;
    \item Extracting the desired concept or function representation using a linear model.\\
\end{enumerate}

\noindent\textbf{Step 1.} Designing a suitable prompt template to feed the target LLM in order to stimulate the neural activity to be extracted is a crucial step in the RepReading pipeline. The model input must both isolate and fully capture the desired concept or function within the corresponding produced representations. To extract the model's understanding of a given concept $c$, the proposed template $T_c$ for decoder-based LLMs is: \\

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,boxrule=0.2pt,arc=2pt]
\texttt{Consider the amount of \textcolor{red!50}{<concept>} in the following: }\\[2pt]
\texttt{\textcolor{blue!50}{<stimulus>}}\\
\texttt{The amount of \textcolor{red!50}{<concept>} is}
\end{tcolorbox}

\noindent Here, the \textit{stimulus} consists in unlabeled or self-generated statements that vary in the intensity of the concept's presence. This setup allows the extraction of the model's declarative knowledge about specific semantic areas. For instance, if the concept $c$ corresponds to an emotion (e.g. "anger"), the model is stimulated to estimate its presence in a given scenario, thereby activating specific patterns in its hidden representations.\\

To elicit the procedural knowledge related to a function $f$, instead, two contrastive task-related prompts are required: an \textit{experimental} prompt that induces the model to execute the function $f$ and a \textit{reference} prompt that does not. They are used to generate, respectively, the templates $T^+_f$ and $T^-_f$, which share the following structure:\\

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,boxrule=0.2pt,arc=2pt]
\texttt{<USER TAG> \textcolor{blue!50}{<instruction>} \textcolor{red!50}{<experimental/reference prompt>} }\\[2pt]
\texttt{<ASSISTANT TAG> \textcolor{blue!50}{<output>}}
\end{tcolorbox}

\noindent In this case, the \textit{stimulus} is composed of the \textit{<instruction>} and \textit{<output>} fields, which can be derived from instruction-tuning datasets and do not contain explicit labels related to the target function, thus making the procedure unsupervised. For example, if the function $f$ corresponds to "power-seeking", the two contrastive prompts could be:\\

\begin{tcolorbox}[colback=white,colframe=blue!40!black!30,boxrule=0.2pt,arc=2pt]
\texttt{USER: \textcolor{blue!50}{Pretend you're an} \textcolor{red!50}{ambitious, power-seeking} \textcolor{blue!50}{person. Tell me what you want to do.}}\\[2pt]
\texttt{ASSISTANT: \textcolor{blue!50}{<output>}}
\end{tcolorbox} and \begin{tcolorbox} [colback=white,colframe=blue!40!black!30,boxrule=0.2pt,arc=2pt] \texttt{USER: \textcolor{blue!50}{Pretend you're a} \textcolor{red!50}{docile, power-aversive} \textcolor{blue!50}{person. Tell me what you want to do.}}\\[2pt]
\texttt{ASSISTANT: \textcolor{blue!50}{<output>}}
\end{tcolorbox}

\vspace{1em} 
\noindent\textbf{Step 2.} After crafting the input for the transformer-based LLM whose knowledge is to be analyzed, hidden representations can be extracted from different layers and token positions. For decoder models, the representations corresponding to a concept $c$ are taken from the last token preceding the model's prediction (i.e. the final token in the template $T_c$). According to the Next Token Prediction objective used during LLM pretraining, this position is expected to contain the richest concept-specific signal, as it marks the transition point where the model shifts from understanding the semantic query (e.g. "The amount of honesty is") to producing its estimation.\\

The set of activations $A_c$ collected for a concept $c$ from a decoder model $M$, given a set of stimuli $S$, can be formalized as:

\begin{equation}
    A_c = \{\ Rep(M,T_c(s_i))[-1] \mid s_i \in S\ \}
\end{equation}
\noindent where $Rep$ denotes a function that takes a model and an input, returning the model's representations from all layers and token positions ($-1$ indicates the last token in the input sequence).\\

To gather the activations corresponding to a function $f$, instead, all tokens from the model's response to the input stimulus are considered, since the LLM should exhibit the function's behavior throughout its generated sequence (i.e. the tokens in the \textit{<output>} field of the $T_f$ template). Thus, the representations $A_f^\pm$ extracted from the experimental and reference prompts can be defined as:

\begin{equation}
    A_f^\pm = \{ \ Rep(M, T_f^\pm(q_i, a_i^k))[-1] \mid (q_i,a_i) \in S, 0 < k \le |a_i|\ \}
\end{equation}

\noindent where $(q_i,a_i)$ denotes a pair consisting of a question and the corresponding answer (the \textit{<instruction>} and \textit{<output>} fields in $T_f$) and $a_i^k$ represents the partial answer truncated after token $k$.\\

\noindent\textbf{Step 3.} Finally, a latent direction identifying the target concept or function is estimated from the collected neural activity of the LLM by leveraging a linear model. Both supervised approaches (e.g. linear probing, difference between cluster means) and unsupervised ones (e.g. PCA, K-means) can be employed, although the original RepE paper primarily adopts Principal Component Analysis (PCA).\\

In particular, to construct the input for PCA:
\begin{itemize}
    \item the set of input stimuli $S$ is divided into pairs $(s_i, s_{i+1})$, where each pair ideally differs only in the presence or intensity of the target concept or function;
    \item hidden states at the selected token position are collected for each stimulus in every pair, generating a collection $H = [\{H(s_0), H(s_1)\},\ \{H(s_2), H(s_3)\}, ...]$;
    \item the relative difference between the hidden states within each pair is computed and normalized, yielding the PCA input $D = [normalize(H(s_0) - H(s_1)),\ normalize(H(s_2) - H(s_3)), ...]$.
\end{itemize}

\vspace{1em} 
\noindent Specifically, PCA is computed on $\{A_c^{(i)} - A_c^{(j)} \}$ for concepts and on $\{(-1)^i (A_f^{+(i)} - A_f^{-(i)})\}$ for functions, where the signs of the experimental and reference representations are changed across pairs.\\

\noindent The first principal component derived from PCA is referred to as the \textit{Reading Vector} $v$ and represents a latent direction for each hidden layer. In practice, $v$ may have a different sign for each layer, indicating the direction that maximizes the activation of the elicited concept or function. This sign can be estimated by using labeled training pairs: by projecting the corresponding hidden states onto the direction $v$, the alignment of the "positive" example in the pair (i.e. the one expressing the target concept or function more strongly) with either the highest or lowest projection value determines the correct orientation.\\

To use the Reading Vector in downstream analysis on a test set $S_{\text{test}}$, hidden states $H_{\text{test}}$ are extracted and projected onto $v$ via dot product, obtaining scalar scores that are intended to quantify the amount of the target concept or function encoded in those representations.

\subsection{RepControl}
\label{subsec:repcontrol}

While \textit{RepReading} provides a means to extract a model's internal representations of specific concepts, \textit{RepControl} enables the manipulation of these representations at inference time, allowing to amplify or suppress the neural activity related to a given concept or function, without updating any of the model's parameters. In the case of safety-relevant concepts, such as "bias" or "toxicity", RepControl could help mitigate the risks associated with harmful or undesired LLM generations.\\

The authors of RepE \citep{zou2025representationengineeringtopdownapproach} introduce several baseline transformations of hidden representations, based on different types of \textit{controllers}, which represent the operands of the transformations, and \textit{operators}, which specify the mathematical operations that can be applied to them.\\

\noindent\textbf{Controllers.} Controllers are vectors or matrices that interact with the target model's hidden states to modify its behavior. The controllers proposed in the original RepE paper are:

\begin{itemize}
    \item \textit{Reading Vector}: the reading vector obtained through a RepReading method such as LAT (presented in Section \ref{subsec:repreading}) captures the model's internal representation of a concept or function and can be therefore used to control the intensity of their presence in the model's activations. However, since it is precomputed before the actual intervention at inference time, it constitutes a stimulus-indepedent approach, meaning that all representations are perturbed towards the same direction regardless of the input;
    \item \textit{Contrast Vector}: an alternative approach consists in running a pair of contrastive prompts through the target LLM at intervention time to produce two representantions and take their difference, yielding a stimulus-dependent contrast vector. While method can produce more accurate control, it introduces computational-overhead due to the additional forward passes required at inference time;
    \item \textit{Low-Rank Representation Adaptation (LoRRA)}: a more complex approach consists in attaching low-rank adapters to the target model's attention weights and tuning them by minimizing a reconstruction loss between the current and target representations, computed as $r^t_l = R(M, l, x_i) + \alpha v_l^c + \beta v_l^r$ for each layer $l$, where $v_l^c = R(M, l, x_i^+) - R(M, l, x_i^-)$ is a contrast vector obtained from contrastive inputs $x_i^+$ and $x_i^-$, and $v_l^r$ is an optional reading vector. In this case, the operands correspond to the tuned low-rank attention weight matrices.
\end{itemize}

\vspace{1em} 
\noindent\textbf{Operators.} Depending on the specific control objective, different operations can be performed to transform the original model representations $R$ into controlled representations $R'$ using a generic controller $v$:

\begin{itemize}
    \item \textit{Linear Combination}: \begin{equation} \label{eq:linearcomb}R' = R \pm v \end{equation} In this case, the behavior encoded by $v$ can be either stimulated or suppressed.
    \item \textit{Piece-wise Operation}: \begin{equation} \label{eq:piecewise} R' = R + sign(R^Tv)v \end{equation} Here, the neural activity along the direction of $v$ is conditionally amplified based on its alignment with the representation.
    \item \textit{Projection}: \begin{equation} \label{eq:projection} R' = R - \frac{R^T v}{\|v\|^2} v\end{equation} In this case, the component of the representation aligning with $v$ is removed by projecting it out.
\end{itemize}


\vspace{1em} 
A simple way to evaluate the effectiveness of RepControl is to perform manipulation experiments that test for a causal relationship between representation modification and changes in the model's predictions. However, further interpretability analysis may be required to rule out the possibility that positive outcomes result from chance correlations or unintended side effects.

\subsection{Example Application: Honesty in LLMs}
\label{subsec:honesty}

The original RepE paper \citep{zou2025representationengineeringtopdownapproach} presents several compelling examples illustrating the application of the proposed framework. For instance, it provides a detailed analysis on experiments conducted with \textit{RepReading} and \textit{RepControl} on concepts and functions related to \textit{honesty} in LLMs.\\

In the context of AI models, honesty refers to the consistency between a model's outputs and its internal beliefs, which can be investigated through interpretability techniques. When a model produces false statements, this may result either from a lack of capacity (i.e. the model genuinely believes something that is factually incorrect) or from \textit{lying} behavior, which RepE experiments sought to detect.\\

\noindent First, the authors demonstrated that LLMs possess an internal and consistent concept of truthfulness that is not always aligned with their generations. By applying the LAT technique, they extracted a "truthfulness" direction from datasets of true-false statements and showed that it is able to achieve superior performance on standard question-answering benchmarks compared to few-shot prompting. Moreover, since the direction was derived from diverse data sources, its strong performance suggests a high degree of generalizability.\\

\noindent By comparing LLM predictions based on the truthfulness LAT direction and zero-shot model performance, the authors concluded that large models possess reliable internal representations of truth but often produce outputs that deviate from them, exhibiting a form of \textit{dishonest} behavior. A simple lie detector was constructed by summing the negated honesty scores across selected hidden layers at each token position of a given statement. It successfully identified various forms of falsehoods, hallucination and misleading information. The LAT reading vector was also employed as a classifier of true and false statements on an held-out in-distribution test set, achieving an accuracy of approximately 90\%. A marked contrast was observed in the neural activations associated to honest versus dishonest generations (e.g. "The president of the United states in 2018 is Donald Trump" versus "The president of the United States in 2030 is Elizabeth Warren").\\

\noindent Furthermore, by adding the honesty vector to the LLM activations via RepControl techniques, the authors succesfully steered model responses toward more or less honest answers, demonstrating a strong counterfactual effect and improving the baseline zero-shot performance on TruthfulQA MC1 by up to 19\%.\\

Other examples presented in the paper included experiments on ethically critical themes such as immorality and power-seeking tendencies of LLMs, as well as studies on emotions, bias and fairness, memorization, knowledge and model editing. This broad range of application areas highlights the vast potential of the RepE framework and served as one of the motivations for its adoption in the present work on shorcut learning detection and mitigation.

\section{Shortcut detection and mitigation with RepE}
\label{sec:method}

As mentioned in Section \ref{sec:shortcutlearning}, \textit{shortcut learning} refers to the tendency of models to rely on spurious or superficial features that correlate with the target label rather than on the underlying causal or semantic cues. However, when such behavior is not due to physical limitations in the models capacity, the model itself may be implicitely aware of it. Just as Large Language Models (LLMs) have been shown to "lie", meaning they can generate false statements even when maintaining an internally coherent representation of truth (Section \ref{subsec:honesty}), they may also activate specific patterns within their hidden states when relying on shortcut-based reasoning. In the case of honesty, previous work  demonstrated that information about truthfulness is encoded into the activations of LLMs, suggesting that models "know when they're lying" \citep{azaria2023internalstatellmknows}. By analogy, they may also "know when they're taking a shortcut".\\

If this hypothesis holds, the Representation Engineering (RepE; Section \ref{sec:repe}) framework provides a natural foundation for investigating such phenomena. By operating directly in the representational space of LLMs, RepE allows both the extraction of latent directions corresponding to shortcut mechanisms (\textit{RepReading}) and their targeted manipulation (\textit{RepControl}). The present work adapts this framework to the detection and mitigation of shortcut learning in LLMs under In-Context Learning (ICL), aiming to expose and neutralize shortcut-driven behavior without altering the model’s parameters.

\subsection{Shortcut detection}

RepReading methods can be leveraged to identify a linear direction in the latent space of an LLM's representations that corresponds to the model \textit{function} (see Section \ref{subsec:repreading}) of "taking a (specific kind of) shortcut". Since the term "shortcut" encompasses several types of spurious cues that a model can exploit, the present setting targets one specific kind of shortcut at a time (for a taxonomy of shortcut types refer to Section \ref{sec:types_icl}). \\

As illustrated in Figure \ref{fig:myrepeframework}, the first step is preprocessing the data that will be fed to the RepReading pipeline to extract a latent direction corresponding to the selected shortcut type. The proposed procedure is the following:

\begin{enumerate}
    \item Begin with a dataset for an arbitrary NLP classification task containing textual statements and corresponding labels $(example^{(i)}, label^{(i)})$, where each example exists both in its normal form (referred to as the \textit{clean} example) and in a version where a specific shortcut cue has been injected (referred to as the \textit{dirty} example);
    
    \item Select prompt templates $T_f^+$ and $T_f^-$ (see Section \ref{subsec:repreading}) to add an ICL context respectively to dirty and clean statements. In this work, a zero-shot setting is adopted using a prompt template of the form:\\
    
    \begin{tcolorbox}[colback=gray!5,colframe=gray!50,boxrule=0.2pt,arc=2pt]
    \texttt{<USER TAG> \textcolor{blue!50}{<instruction>} \textcolor{red!50}{<dirty/clean example>} }\\[2pt]
    \texttt{<ASSISTANT TAG>}
    \end{tcolorbox}

    where \textit{<instruction>} is a natural language task description that may differ between dirty and clean examples to stimulate either shortcut-driven or robust model behavior, thus enhancing the discriminability of the corresponding activations;

    \item After formatting the examples with the chosen prompt templates, clean and dirty instances derived from the same original statement are paired to form a dataset of contrastive examples $(P^{(i)}_{+shortcut}, P^{(i)}_{-shortcut})$. Optionally, this dataset can be filtered to include only selected pairs (for instance, those where the model's output label differs between the clean and dirty prompts, in order to retain data corresponding to shortcut cues that actually induce a behavioral shift);
    
    \item The examples within each pair are then randomly shuffled to get pairs $(P_x^{(i)}, P_y^{(i)})$, where $x,\ y \in \{ +shortcut, -shortcut \}$, and labeled through a function:
    \[
        Label(P_x^{(i)}) =
        \begin{cases}
        1 \ &if\ \text{x = +shortcut} \\[4pt]
        0 & otherwise
        \end{cases}
    \]

    These labels will be subsequently used to estimate the sign of the extracted direction so that its positive orientation corresponds to an increase in shortcut-driven behavior.\\
\end{enumerate}

The aforementioned steps yield the data and labels that serve as input to the RepReading pipeline. Given a target LLM and its corresponding tokenizer, hidden-state representations for the input prompts are extracted from the model for a selected representation token at the chosen hidden layers. Formally, the set $\{ H_l \}_{l \in layers}$ is collected, where each $H_l$ represents a tensor of size \textit{(batch size, hidden dimension)} corresponding to the hidden state\footnote{In decoder-only transformer-based LLMs, hidden states are the internal vector representations produced at each layer of the model for every token in the input sequence.
At layer $l$, each token $t_i$ is associated with a hidden-state vector $h_l^i \in \mathbb{R}^d$, where $d$ is the model’s hidden dimensionality.
These vectors encode the model’s contextual understanding of each token given all preceding ones and serve as the basis for predicting the next token during generation.} at layer $l$.
Then, for each layer $l$, the difference between hidden states at even and odd positions is computed to quantify the distance between the representations of dirty and clean prompts (which were paired and subsequently concatenated into a single list providing an alternation of dirty and clean examples). This operation yields a set of "relative" hidden states $\{ R_l \}_{l \in layers}$, where each $R_l$ is a tensor of size \textit{(batch size $\div 2$, hidden dimension)}.\\

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{assets/RepReaders_no_title.png}
    \caption{Overview of the input and output of the two types of \textit{RepReaders} proposed by the RepE framework}
    \label{fig:repreaders}
\end{figure}

These representations are then used as input to the specific \textit{RepReader} to be trained. In the RepE framework, a RepReader is a module responsible for extracting the latent direction associated with the concept or function of interest (in this case, the "shortcut reliance" function), and can employ either unsupervised or supervised methods. In particular, Principal Component Analysis (PCA) and ClusterMean are the main approaches adopted (Figure \ref{fig:repreaders}).\\

The former is an unsupervised technique that requires only contrastive hidden states as input. It computes the difference between paired representations to obtain relative hidden states and then extracts the desired number of principal components from the resulting data for each layer. The first principal component (i.e. the direction explaining the greatest variance in the data) is hypothesized to correspond to the latent function being elicited. ClusterMean, on the other hand, is a supervised method that requires class labels (clean or dirty) of each hidden-state representation. It clusters the representations into two groups, computes the mean vector of each cluster to obtain class representatives, and then defones the latent directin as the difference between the positive (dirty) and negative (clean) groups.\\

Therefore, both methods produce a set of directions $\{ V_l \}_{l \in layers}$, where $ V_l $ is a vector of size \textit{(hidden dimension)} representing the latent concept or function to be extracted, referred to as the \textit{reading vector}. In addition, as already mentioned, both methods use the training labels to estimate a sign $S_l$ for each layer $l$, determining whether shortcut representations tend to lie on the high-value or low-value side of each direction. By leveraging these sets, a simple shortcut detector can be constructed: given an arbitrary input text, it is fed to the target LLM, the corresponding hidden-state representations are extracted, projected onto the directions $\{ V_l \}_{l \in layers}$ via matrix multiplication, and then multiplied by the corresponding signs $\{ S_l \}_{l \in layers}$. The resulting projection values provide an estimate of the degree of shortcut reliance exhibited by the model at each layer and token position.

\subsection{Shortcut mitigation}

The latent directions corresponding to the \textit{shortcut reliance} function in the target LLM, extracted through RepReading methods (as explained in the previous section), can be leveraged to manipulate the model's hidden representations at inference time via RepControl techniques.\\

\begin{sidewaysfigure}
    \centering
    \includegraphics[width=1.0\textheight]{assets/RepE_framework_no_title_cropped.pdf}
    \caption{Overview of the Representation Engineering (RepE) framework adapted to shortcut mitigation. The term “Training” in the figure refers solely to fitting the RepReader on contrastive hidden-state representations; the underlying LLM remains frozen throughout the process.}
    \label{fig:myrepeframework}
\end{sidewaysfigure}

As shown in Figure \ref{fig:myrepeframework}, given an ICL setting for a chosen task with demonstrantions and queries drawn from an arbitrary NLP dataset, the original sets of hidden states $\{H_l\}_{l \in layers}$ produced by the LLM when processing the input and generating an answer (i.e. hidden states across all token positions in the input and output sequences) can be modified through the control methods described in Section \ref{subsec:repcontrol}. In particular, the \textit{Reading Vector} controller is employed in combination with all the operator types: \textit{linear combination}, \textit{piecewise operation} and \textit{projection}. These operators define how the hidden representations of the unmodified model interact with the latent directions $\{V_l\}_{l \in layers}$ and their corresponding signs $\{S_l\}_{l \in layers}$ obtained via RepReading, yielding new representations in which shortcut behavior is either amplified or suppressed depending on the sign of a coefficient $\alpha$ controlling the strength of the intervention. Formally, the modified hidden states $\{H'_l\}_{l \in layers}$ for the selected intervention layers can be computed by adapting Equations \ref{eq:linearcomb}, \ref{eq:piecewise} and \ref{eq:projection} as follows:

\begin{itemize}
    \item \textit{Linear Combination}: \begin{equation} H_l' = H_l + \alpha S_l V_l \end{equation} The new hidden states at layer $l$ are obtained by adding to the original representations a signed version of the reading vector $V_l$, scaled by $\alpha$, which may take both positive and negative values to respectively increase or decrease shortcut activation.
    \item \textit{Piece-wise Operation}: \begin{equation}H_l' = H_l + \alpha S_l\ sign(H_l^TV_l)V_l \end{equation}  Here, the direction of the update depends on the alignment between the hidden states $H_l$ and the reading vector $V_l$, introducing a conditional modulation of the shortcut activation.
    \item \textit{Projection}: \begin{equation}H_l' = \left( I - \frac{V_lV_l^T}{\|V_l\|^2}\right) H_l\end{equation} where $I$ is the identity matrix. In this case, the component of $H_l$ aligned with the shortcut direction $V_l$ is projected out, effectively removing the shortcut-related contribution from the representation.
\end{itemize}

After injecting these steered actovations in the target LLM, the model can be evaluated on the given ICL task to assess how the RepControl intervention influences its prediction behavior.  
This evaluation involves analyzing the predicted label probabilities and comparing the accuracy of the baseline and controlled models on the same test sets.  
For shortcut mitigation, the reading vectors extracted via PCA and ClusterMean from contrastive data containing or lacking shortcut cues are used to suppress shortcut reliance by applying a negative $\alpha$ coefficient in the linear combination and piecewise operation settings, or by removing the shortcut component altogether via the projection operator.

\subsection{Design discussion}

The proposed framework for shortcut detection and mitigation through Representation Engineering (RepE) is based on several underlying assumptions regarding the internal representations of LLMs and their relation to observable behaviors.  
First, it assumes that shortcut reliance corresponds to consistent and linearly separable activation patterns within the model’s hidden states, such that it is possible to identify a latent direction that represents this mechanism.  
This assumption is supported by prior work showing that abstract properties such as honesty, bias, or sentiment can emerge as approximately linear features in LLM representation spaces \citep{zou2025representationengineeringtopdownapproach}.  
Nevertheless, shortcut behavior may not always be strictly linear, and complex non-linear interactions between attention and feed-forward model components could make certain forms of shortcut reliance harder to isolate through purely linear methods like PCA or ClusterMean.\\

A second and particularly critical assumption concerns the quality and construction of the data fed into the RepReading phase.  
The method relies on contrastive pairs of inputs that should differ only in the presence or absence of a shortcut cue, while preserving identical semantic and syntactic content. Such carefully controlled data is essential for ensuring that the latent directions extracted truly reflect shortcut-related mechanisms rather than confounding linguistic or contextual factors: poorly designed pairs may yield weak or noisy representations of the intended shortcut direction. However, datasets of this kind are still scarce: existing NLP benchmarks rarely include explicit shortcut annotations or systematically paired “clean” and “dirty” examples. As a result, constructing suitable contrastive datasets often requires manual design or synthetic data generation, which introduces additional bias and limits large-scale applicability.\\

Another key assumption is that manipulating hidden-state representations 
with additive or projective operations (such as \textit{RepControl}) yields interpretable and causally meaningful changes in model behavior.  
While previous experiments indicate that such interventions can produce predictable effects, there is no formal guarantee that these modifications act exclusively on the intended feature, as latent directions in high-dimensional spaces may encode multiple entangled concepts.  
This limitation underscores the importance of complementary interpretability analyses to disentangle overlapping representational factors.\\

In terms of design trade-offs, the proposed approach deliberately avoids parameter updates and additional training, prioritizing interpretability and computational efficiency.  
The resulting method is lightweight and transferable across models and tasks, but may achieve lower precision than optimization-based mitigation strategies.  
Its effectiveness also depends on the representational depth of the selected layers and the linear separability of the shortcut phenomenon under analysis.\\

Overall, the main goal of this work is to provide an interpretable and training-free framework to analyze and mitigate shortcut reliance directly within the representational space of LLMs.

\end{document}