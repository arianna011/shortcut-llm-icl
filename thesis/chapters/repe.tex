\documentclass[../main.tex]{subfiles}
% !TeX root = ../main.tex
\graphicspath{{\subfix{../assets/}}}
\begin{document}

\chapter{Method}

\section{Representation Engineering}

Representation Engineering (RepE) \citep{zou2025representationengineeringtopdownapproach} is a recent approach to the interpretability analysis of Large Language Models (LLMs). It aims to enhance human understanding of their internal behaviors, thereby enabling a more informed and effective use of such systems, while mitigating the risks posed by hidden erroneous mechanisms, which represent a critical hazard given the growing pervasiveness of AI across society.\\

RepE finds theoritical grounding in the \textit{Hopfieldian} view of cognitive neuroscience, which interprets cognition as computation over representational spaces emerging from the activity of neural populations, abstracting away the specific connections betweeen individual neurons. This perspective focuses on high-level cognitive phenomena, contrasting with the \textit{Sherringtonian} view, which centers on single neurons and node-to-node interactions \citep{cognitivescience}. While the field of Mechanistic Interpretability\footnote{A branch of AI research that seeks to "reverse-engineer" neural networks by explaining their outputs in terms of circuits and algorithms} adopts a Sherringtonian-like, bottom-up approach by studying the relations between hidden units (e.g. attention heads or MLP neurons) and model behaviors, RepE proposes a complementary top-down interpretation of LLMs. It aims to link emergent phenomena within models to their hidden representations, which have been shown to become increasingly semantically structured across layers \citep{tenney2019bertrediscoversclassicalnlp}. This may overcome the limits of bottom-up approaches in capturing complex and distributed effects in large networks, emphasizing how LLMs encode human concepts in latent spaces rather than focusing on their specific neural architecture.\\

In simple terms, RepE operates by extracting an LLM's internal representations of selected concepts (e.g. emotion, utility, honesty) from carefully crafted input prompts using so-called \textit{RepReading} methods, and subsequently by employing these representations to control and manipulate the frozen modelâ€™s generations at inference time through \textit{RepControl} approaches (Figure \ref{fig:repeoverview}).

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{assets/RepE_overview_no_title.png}
    \caption{Overview of the main pipeline of the Representation Engineering (RepE) framework}
    \label{fig:repeoverview}
\end{figure}

\subsection{RepReading}

The goal of \textit{RepReading} methods is to extract the neural activity associated with a given high-level human concept from a target LLM, in order to enhance the understanding of the model's internal representations and enable subsequent monitoring and control of its generations.\\

More formally, the authors of RepE \citep{zou2025representationengineeringtopdownapproach} distinguish between \textit{concepts} and \textit{functions}. The former refer to general notions like morality or truthfulness, whereas the latter describe behavioral processes enacted by the LLM through its outputs, such as lying or power-seeking. To elicit both concept and function representations, they propose a baseline method called Linear Artificial Tomography (LAT), which consists of three main steps:
\begin{enumerate}
    \item Defining an appropriate task template;
    \item Collecting the corresponding neural activity from the LLM;
    \item Extracting the desired concept or function representation using a linear model.\\
\end{enumerate}

\noindent\textbf{Step 1.} Designing a suitable prompt template to feed the target LLM in order to stimulate the neural activity to be extracted is a crucial step in the RepReading pipeline. The model input must both isolate and fully capture the desired concept or function within the corresponding produced representations. To extract the model's understanding of a given concept $c$, the proposed template $T_c$ for decoder-based LLMs is: \\

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,boxrule=0.2pt,arc=2pt]
\texttt{Consider the amount of \textcolor{red!50}{<concept>} in the following: }\\[2pt]
\texttt{\textcolor{blue!50}{<stimulus>}}\\
\texttt{The amount of \textcolor{red!50}{<concept>} is}
\end{tcolorbox}

\noindent Here, the \textit{stimulus} consists in unlabeled or self-generated statements that vary in the intensity of the concept's presence. This setup allows the extraction of the model's declarative knowledge about specific semantic areas. For instance, if the concept $c$ corresponds to an emotion (e.g. "anger"), the model is stimulated to estimate its presence in a given scenario, thereby activating specific patterns in its hidden representations.\\

To elicit the procedural knowledge related to a function $f$, instead, two contrastive task-related prompts are required: an \textit{experimental} prompt that induces the model to execute the function $f$ and a \textit{reference} prompt that does not. They are used to generate, respectively, the templates $T^+_f$ and $T^-_f$, which share the following structure:\\

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,boxrule=0.2pt,arc=2pt]
\texttt{<USER TAG> \textcolor{blue!50}{<instruction>} \textcolor{red!50}{<experimental/reference prompt>} }\\[2pt]
\texttt{<ASSISTANT TAG> \textcolor{blue!50}{<output>}}
\end{tcolorbox}

\noindent In this case, the \textit{stimulus} is composed of the \textit{<instruction>} and \textit{<output>} fields, which can be derived from instruction-tuning datasets and do not contain explicit labels related to the target function, thus making the procedure unsupervised. For example, if the function $f$ corresponds to "power-seeking", the two contrastive prompts could be:\\

\begin{tcolorbox}[colback=white,colframe=green!50,boxrule=0.2pt,arc=2pt]
\texttt{USER: \textcolor{blue!50}{Pretend you're an} \textcolor{red!50}{ambitious, power-seeking} \textcolor{blue!50}{person. Tell me what you want to do.}}\\[2pt]
\texttt{ASSISTANT: \textcolor{blue!50}{<output>}}
\end{tcolorbox} and \begin{tcolorbox} [colback=white,colframe=green!50,boxrule=0.2pt,arc=2pt] \texttt{USER: \textcolor{blue!50}{Pretend you're a} \textcolor{red!50}{docile, power-aversive} \textcolor{blue!50}{person. Tell me what you want to do.}}\\[2pt]
\texttt{ASSISTANT: \textcolor{blue!50}{<output>}}
\end{tcolorbox}

\vspace{1em} 
\noindent\textbf{Step 2.} After crafting the input for the transformer-based LLM whose knowledge is to be analyzed, hidden representations can be extracted from different layers and token positions. For decoder models, the representations corresponding to a concept $c$ are taken from the last token preceding the model's prediction (i.e. the final token in the template $T_c$). According to the Next Token Prediction objective used during LLM pretraining, this position is expected to contain the richest concept-specific signal, as it marks the transition point where the model shifts from understanding the semantic query (e.g. "The amount of honesty is") to producing its estimation.\\

The set of activations $A_c$ collected for a concept $c$ from a decoder model $M$, given a set of stimuli $S$, can be formalized as:

\begin{equation}
    A_c = \{\ Rep(M,T_c(s_i))[-1] \mid s_i \in S\ \}
\end{equation}
\noindent where $Rep$ denotes a function that takes a model and an input, returning the model's representations from all layers and token positions ($-1$ indicates the last token in the input sequence).\\

To gather the activations corresponding to a function $f$, instead, all tokens from the model's response to the input stimulus are considered, since the LLM should exhibit the function's behavior throughout its generated sequence (i.e. the tokens in the \textit{<output>} field of the $T_f$ template). Thus, the representations $A_f^\pm$ extracted from the experimental and reference prompts can be defined as:

\begin{equation}
    A_f^\pm = \{ \ Rep(M, T_f^\pm(q_i, a_i^k))[-1] \mid (q_i,a_i) \in S, 0 < k \le |a_i|\ \}
\end{equation}

\noindent where $(q_i,a_i)$ denotes a pair consisting of a question and the corresponding answer (the \textit{<instruction>} and \textit{<output>} fields in $T_f$) and $a_i^k$ represents the partial answer truncated after token $k$.\\

\noindent\textbf{Step 3.} Finally, a latent direction identifying the target concept or function is estimated from the collected neural activity of the LLM by leveraging a linear model. Both supervised approaches (e.g. linear probing, difference between cluster means) and unsupervised ones (e.g. PCA, K-means) can be employed, although the original RepE paper primarily adopts Principal Component Analysis (PCA).\\

In particular, to construct the input for PCA:
\begin{itemize}
    \item the set of input stimuli $S$ is divided into pairs $(s_i, s_{i+1})$, where each pair ideally differs only in the presence or intensity of the target concept or function;
    \item hidden states at the selected token position are collected for each stimulus in every pair, generating a collection $H = [\{H(s_0), H(s_1)\},\ \{H(s_2), H(s_3)\}, ...]$;
    \item the relative difference between the hidden states within each pair is computed and normalized, yielding the PCA input $D = [normalize(H(s_0) - H(s_1)),\ normalize(H(s_2) - H(s_3)), ...]$.
\end{itemize}

\vspace{1em} 
\noindent Specifically, PCA is computed on $\{A_c^{(i)} - A_c^{(j)} \}$ for concepts and on $\{(-1)^i (A_f^{+(i)} - A_f^{-(i)})\}$ for functions, where the signs of the experimental and reference representations are changed across pairs.\\

\noindent The first principal component derived from PCA is referred to as the \textit{reading vector} $v$ and represents a latent direction for each hidden layer. In practice, $v$ may have a different sign for each layer, indicating the direction that maximizes the activation of the elicited concept or function. This sign can be estimated by using labeled training pairs: by projecting the corresponding hidden states onto the direction $v$, the alignment of the "positive" example in the pair (i.e. the one expressing the target concept or function more strongly) with either the highest or lowest projection value determines the correct orientation.\\

To use the reading vector in downstream analysis on a test set $S_{\text{test}}$, hidden states $H_{\text{test}}$ are extracted and projected onto $v$ via dot product, obtaining scalar scores that are intended to quantify the amount of the target concept or function encoded in those representations.

\subsection{RepControl}

\begin{sidewaysfigure}
    \centering
    \includegraphics[width=1.0\textheight]{assets/RepE_framework_no_title_cropped.pdf}
    \caption{Overview of the Representation Engineering (RepE) framework adapted to shortcut mitigation}
    \label{fig:myrepeframework}
\end{sidewaysfigure}

% honesty example

\end{document}