\documentclass[../main.tex]{subfiles}
% !TeX root = ../main.tex
\graphicspath{{\subfix{../assets/}}}
\begin{document}

\chapter{Background \& Related Work}
\label{chap1}

\section{In-Context Learning }
Large Language Models (LLMs) are the most successful class of AI systems currently employed in Natural Language Processing (NLP). They serve as modern assistants across several applications, ranging from open-domain conversation and question answering to machine translation and code generation. Their rapid progress has been largely driven by scaling: architectures with increasing depth and width, trained on massive and diverse web-scale corpora, have not yet shown signs of performance saturation. In fact, they follow empirical scaling laws that predict consistent improvements with greater model size, data and compute \citep{kaplan2020scalinglawsneurallanguage}. The main limitation to further progress is therefore not intrinsic model capability, but rather the economic and environmental costs of training ever-larger architectures on ever-broader datasets. 
\\

% ICL
An interesting consequence of such large-scale pretraining was the emergence of the In-Context Learning (ICL) paradigm \citep{brown2020languagemodelsfewshotlearners}. LLMs demonstrated the surprising ability to perform novel tasks after being provided only a few input-output demonstrations, presumably inferring the underlying semantic pattern in a similar way to how humans would solve new problems after looking at some examples. LLMs such as GPT-3 \citep{brown2020languagemodelsfewshotlearners}, Llama and OPT \citep{milios2023incontextlearningtextclassification} are able to achieve state-of-the-art performance on several NLP benchmarks when evaluated in ICL settings, even competing with finetuned task-specific models.

\subsection{Definition}

An LLM models a probability distribution $p_\theta(w_t|w_{<t})$ over the text tokens $w_i$ in its vocabulary $V$ in order to build coherent natural language sequences ($w_1w_2 ...w_t...$). In transformer-based architectures \citep{vaswani2023attentionneed}, the parameters $\theta$ learned for attention and feed-forward units ultimately determine the likelihood of each token being the next word in the sequence, conditioned on all preceding ones (\textit{autoregression}). A decoding strategy, either deterministic (e.g. greedy choice) or stochastic (e.g. temperature-based sampling), is then applied to effectively select a new token and continue the text generation process.\\

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{assets/ICL_new_no_title.png}
    \caption{Representation of the In-Context Learning (ICL) paradigm for LLMs}
    \label{fig:icl}
\end{figure}

In an ICL setting (Figure \ref{fig:icl}), given an NLP task defining an input-label relationship function $f: V^* \to Y$ (where $V^*$ denotes the space of all possible text sequences and $Y = \{y_1,...,y_m\}$ is the set of valid answers or labels required by the task), an arbitrary number $k$ of example pairs $D = \{(x_i, f(x_i))\}_{i=1}^k$ can be selected from a dataset to serve as task demonstrations for an LLM that has not been explicitly trained to perform that task. The prompt that is presented to the LLM is obtained by populating a chosen prompt template $P(I,D,Q)$, which specifies how to format the optional task instruction $I$ (written in natural language), the demonstrations set $D$ and the query $Q = x_q$, which represents the the task instance for which the LLM is required to produce an answer $y_q^{pred}$.\\


Without any parameters update, the frozen LLM leverages its vast and generalizable prior knowledge to try to infer the hidden function $f$ from the demonstrations.
The predicted answer is computed as:
\[
y_{q}^{pred} = \arg\max_{y\in Y} \; p_{\theta}(y \mid P(I, D,Q))
\]

\noindent When the number of demonstrations is $k = 0$, the model operates in a \textit{zero-shot} setting, relying solely on the task instruction. For $k > 0$, the typical \textit{few-shot} setting is established, where contextual examples guide the model's predictions.

\subsection{Influencing factors \label{sec:ICL_factors}}

Several studies have analyzed the empirical factors that can influence LLM performance in ICL settings \citep{dong2024surveyincontextlearning}. At the pretraining stage, the diversity of source domains within the training corpora has been shown to significantly enhance the ability of LLMs to generalize to unseen tasks in ICL, often even more than the overall corpus size. However, while pretraining on a dataset specifically related to a downstream task seems to help zero-shot learning performance, it does not always guarantee competitive few-shot results on the same task. Also, ICL capabilities can emerge when a model is trained on a combination of multiple corpora, even if each corpus individually does not promote ICL by itself \citep{shin-etal-2022-effect}. Beyond data quality and diversity, both the model architecture and the training process can influence ICL performance: a larger number of parameters or training steps tends to correlate with stronger in-context learning abilities \citep{wei2022emergentabilitieslargelanguage}.\\

At inference time, LLMs proved to be very sensitive to several features of the ICL prompt itself. First, the selection of the demonstration examples predictably influences ICL performance. Demonstrations drawn from out-of-distribution (OOD) data with respect to the pretraining corpus generally yield lower results \citep{min2022rethinkingroledemonstrationsmakes}. Moreover, better performance is typically achieved when demonstrations have embeddings that are semantically closer to that of the query \citep{liu2021makesgoodincontextexamples}. Factors such as the format used to present demonstrations, the coverage of the label distribution and the order in which examples are arranged can all affect model predictions. For instance,  \citep{lu2022fantasticallyorderedpromptsthem} observed that certain permutations of examples lead to substantially higher performance than others, but found no consistent pattern among effective orderings, nor any transferability across different model sizes or tasks. Such feature sensitivities, which can be interpreted as a form of inductive bias \citep{si2023measuringinductivebiasesincontext}, reduce the interpretability of ICL mechanisms and will be further discussed in Section \ref{sec:types_icl}.

\subsection{Theoretical explanations}
To date, it is still not entirely clear why and how ICL works. Some argue that, during pretraining, an LLM is exposed to implicit task demonstrations that can later be triggered and reused in an ICL setting, while others suggest that LLMs are capable of learning new input-label mappings directly from the contextual examples. In \citep{pan2023incontextlearninglearnsincontext}, the authors propose to decouple ICL into Task Recognition (TR) and Task Learning (TL). The former refers to recognizing a previously encountered task from demonstrations and applying relevant pretraining priors; thus, it does not even require a correct association between text and label in the examples. The latter, instead, involves genuinely learning a new input-label mapping from the provided demonstrations and therefore relies on accurate ground-truth labels. The study shows that, while TR emerges as a broad capability across model scales, TL is enabled when increasing both the number of model parameters and the number of in-context examples, making larger models substantially more capable.\\

When adopting a Bayesian framework, ICL can be interpreted as a Bayesian inference process implemented in the forward pass of the LLM transformer:
\[
p_{\theta}(y \mid D, x) = \int p(y \mid x, \phi
) \ p_\theta(\phi \mid D) \ d\phi\]

\noindent Given the set of contextual demonstrations $D = \{(x_i,f(x_i))\}_{i=1}^k$ and the query input $x$, the model's output distribution is obtained by integrating the conditional likelihoods $p(y \mid x, \phi)$ of possible answers $y$ under a latent task hypothesis $\phi$, weighted by their posterior probabilities $p_\theta(\phi \mid D)$. This formulation can be interpreted as a form of Bayesian Model Averaging (BMA) \citep{zhang2023doesincontextlearninglearn}. 
In this view, the LLM implicitly maintains a distribution over possible task hypotheses $\phi$, represented by $p_{\theta}(\phi \mid D)$, which is inferred from the in-context examples $D$ and encoded within its hidden representations, rather than explicitly computed. Hence, the LLM acts as a \textit{metalearner}\footnote{a model that "learns to learn" through its internal representations} by inferring a posterior over latent task hypotheses and averaging their corresponding predictions to generate an answer.\\

Other interpretations draw parallels between ICL and gradient-based optimizations, suggesting that LLMs may be encoding smaller models in their activations and updating such implicit models as new examples appear in the ICL context.
\citep{vonoswald2023transformerslearnincontextgradient} demonstrated that a single attention layer can emulate a gradient descent update on a simple regression loss, and that deeper transformers effectively perform multiple such updates across layers, behaving as implicit optimizers. 
Similarly, \citep{akyürek2023learningalgorithmincontextlearning} showed that transformers can instantiate classical algorithms, such as gradient descent, ridge regression, or closed-form least squares, by encoding model parameters within hidden representations and updating them dynamically as new examples are processed. 

\subsection{Advantages and limitations}
Beyond emulating the human ability to learn from examples, ICL provides an interpretable interface to LLMs, as both instructions and demonstrations are expressed in natural language. It also requires significantly lower computational costs compared to supervised learning, since it does not involve parameter optimizations but only leverages LLMs prior knowledge to generalize to new task patterns. While ICL alone can achieve strong performance, it can be further enhanced through specialized training strategies or pre-inference warm-up to adjust model activations. \\

These advantages have encouraged the application of ICL not only to traditional NLP tasks, but also to areas such as data engineering, where it enables the generation of annotated datasets with minimal human supervision, and retrieval-based model augmentation and knowledge updating, in which external information is injected into an LLM via the in-context prompt.\citep{dong2024surveyincontextlearning}. 
\\

On the downside, as mentioned in Section \ref{sec:ICL_factors}, the performance of ICL is highly sensitive to multiple prompt-related factors, such as example order and formatting, which can reduce its effectiveness and demand careful prompt engineering. Moreover, the underlying mechanisms of ICL remain only partially understood and require further investigation to be fully exploited. Such sensitivities and limited interpretability raise the question of whether LLMs truly learn task semantics in-context or merely rely on spurious cues and surface patterns to make their predictions, a behavior known as \textit{shortcut learning}, which finds particularly fertile ground in ICL settings.\\

\section{Shortcut Learning }
\label{sec:shortcutlearning}
Machine Learning (ML) models have shown impressive results across a wide range of domains, from natural language processing to computer vision and multimodal applications. However, when faced with a distributional shift between training datasets and real-world data, these models still often fail to generalize reliably. Instead of learning a robust causal relation between input features and task outputs, they frequently exploit simple spurious correlations embedded in the training set to make their predictions, which can result in unintuitive failures on out-of-distribution examples.

\begin{figure}[h]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=1.1\linewidth]{assets/[cow].png}
        \caption{Example of a CNN leveraging a dataset bias as a shortcut: since cows only appear on grassy backgrounds in the training data, the model learns to predict "cow" when it recognizes large green areas in the image and is not able to recognize the animal in a different setting}
        \label{fig:cow}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/LLM shortcut.png}
        \caption{Example of an LLM learning a shortcut behavior for the Argument Reasoning Comprehension Task (ARCT)}
        \label{fig:reasoning}
    \end{minipage}
    %\caption{Two images side by side.}
    \label{fig:two_side}
\end{figure}

For instance, Convolutional Neural Networks (CNNs) have shown a tendency to classify objects based on superificial cues in the image (e.g. the background the object mostly appears on in training data) rather than on their actual characterizing features (Figure \ref{fig:cow}) \citep{beery2018recognitionterraincognita}. A critical real-world example was represented by a classifier trained to detect pneumonia from images of chest X-ray scans. The model achieved high accuracy by exploiting hospital-specific watermarks, effectively ignoring the actual pathological signals in the images  \citep{Zech2018VariableGP}. In general, ML models may leverage any feature observed during training to make predictions, leading to issues when \textit{dataset bias}\footnote{\textit{Dataset bias} occurs when the training data fails to accurately represent the real-world distribution in which the model will operate (for instance, when a classifier is trained only on images of cows standing on grass)} is present: object textures or even imperceptible high-frequency patterns could become unintended label predictors. \\

Such behavior is not confined to computer vision. Large Language Models (LLMs) have also been shown to rely on spurious cues (e.g. occurrence of high-frequency words such as “not”) to perform tasks, rather than genuinely understanding semantic relationships. In the Argument Reasoning Comprehension Task (ARCT), for instance, BERT selected the correct answer primarily by exploiting lexical shortcuts (Figure \ref{fig:reasoning}) \citep{niven-kao-2019-probing}. Similarly, in agent-based reinforcement learning, models can reach high scores without learning the intended behavior, by "hacking" the reward function. A known example is the algorithm that learned to pause Tetris indefinitely to avoid losing, without actually learning how to play the game \citep{Murphy2013TheFL}.\\

Such phenomena illustrate how models may confuse correlation (the circumstantial occurrence of some features within specific data categories) with causation (the underlying real-world attributes that truly define entities), achieving high apparent performance while lacking genuine understanding. They represent a major vulnerability of ML models, as they undermine both robustness and interpretability. Moreover, they raise significant societal concerns: decision-making system, such as Amazon’s hiring tool, can base their predictions on sensitive attributes like race or gender, thus perpetuating or even amplifying existing social biases and resulting in unfair outcomes \citep{zhao2017menlikeshoppingreducing}. 


\subsection{Definition}
The term "\textit{shortcut learning}" refers to the tendency of ML models to rely on non-robust decision rules (shortcuts), that allow them to perform well on test sets with the same distribution of training data but reveal their weakness under out-of-distribution (OOD) testing or adversarial attacks.\\

Shortcut learning behaviors have been also observed in biological systems. Animals can find unexpected simple solutions to experimental tasks designed to study specific cognitive abilities (\textit{unintended cue learning}), while human students may adopt superificial learning strategies based on rote memorization, achieving good scores without developing transferable understanding (\textit{surface learning}) \citep{Geirhos_2020}.
One of the earliest and most famous examples of shortcut learning was the case of Clever Hans, a horse living in the early 20th century that appeared capable of performing arithmetic operations and other intellectual tasks but was in fact responding to involuntary body cues from its human trainer (\textit{Clever Hans effect}) \citep{cleverhans}.\\

In general, shortcuts represent an appealing yet deceptive deviation from the intended solution to a given problem. Since most real-world problems are too complex to be explicitely formalized, ML models are designed to autonomously approximate their solutions from examples. However, when learning a predictive function 
\begin{equation*}
f_{pred}: F_{input} \to Y
\end{equation*} 

\noindent that maps input data features $F_{input}$ to task labels $Y$, an ML model could assign importance to different features from those a human would intuitively consider as relevant for the same task. In particular, the model might attribute a high weight to a feature $f_i \in F_{input}$ that exhibits a strong correlation $c(f_i, y_j)$ with a label $y_j \in Y$, even if there is no causal relation between them. Such correlations are called "spurious" and can be either \textit{world-induced}, when arising from the ground truth data distribution $P_{gt}(x)$ (e.g. cows statistically appear more often on grass than on sand) or \textit{sampling-induced}, when resulting from selection biases in the training dataset $\{x_i\}_{i=1}^N$ drawn from the observed distribution $P(x)$, which inevitably deviates from the true one \citep{steinmann2024navigatingshortcutsspuriouscorrelations}. \\

When an ML model relies on such spurious correlations for its predictions instead of capturing more complex input-label relationsihps, it effectively learns to take a "shortcut". This leads it to generalize unpredictably on OOD data, often revealing a mismatch between the ground-truth function $f_{gt}$ and its learnt approximation $f_{pred}$. In the object classification example, an ML model might fail at recognizing objects under seemingly innocuous distribution shifts, such as changes in rotation or illumination, while simultaneously detecting specific object classes in images where humans would not perceive them at all (Figure \ref{fig:gen}).\\

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{assets/generalization.png}
    \caption{A generic Deep Neural Network (DNN) trained to recognize objects from images may fail to detect classes such as "cow" in visual contexts not observed during training, or, conversely, rely on irrelevant features to predict their presence in images where the object is absent}
    \label{fig:gen}
\end{figure}

\subsection{Causes}
One of the principal reasons for shorcut learning is the \textit{simplicity bias} of ML models: they tend to fit data with simplest available function, aiming to achieve the best performance with the least effort \citep{vallepérez2019deeplearninggeneralizesparameterfunction}. While this often helps prevent overfitting on training datasets, it can also lead to overlook more informative yet complex data patterns.\\

In practice, the causes of shorcut learning can be found both within ML models and in the data they are fed. The set of solutions learnable by a model is determined by its \textit{inductive bias}, which represents its intrinsic prior assumptions and influences the type of function it will prefer to learn given finite capacity. The inductive bias is, in turn, determined by multilple components: the model's architecture (e.g. convolutions promot feature detection robust to spatial location within an image), the loss function formalizing the learning objective (e.g. cross-entropy without regularization may encourage reliance on a few shortcut features), the optimization process (e.g. stochastic gradient descent is inherently susceptible to simplicity bias, especially with high learning rates) and the training data itself, which can be imbalanced or contain annotation artifacts \citep{Geirhos_2020}. Moreover, shortcut features often exhibit less noise than the truly relevant ones within the training distribution, making them appear more predictive for the target task.\\

In the case of LLMs, both the pretraining and finetuning data play a critical role in the emergence of shortcut learning. For instance, LLMs have been shown to produce predictions biased towards high-frequency and frequently co-occurring words observed during training. Somewhat counterintuitively, increasing model size does not mitigate shorcut learning: larger LLMs are more prone to exploiting shortcuts, likely because their increased capacity allows them to memorize a broader range of spurious cues \citep{song2024shortcutlearningincontextlearning}. Conversely, longer fine-tuning may promote better generalization, since shortcuts are often learned during the early steps of training \citep{du2023shortcutlearninglargelanguage}.

\subsection{Shortcut types in ICL}
\label{sec:types_icl}

This work focuses on shortcut learning in natural language processing (NLP) tasks within in-context learning (ICL) settings, where LLMs are likely to rely on both \textit{instinctive} shorcuts, which represent inherent model preferences absorbed during pretraining, and \textit{acquired} shortcuts, which emerge from the specific prompt demonstrations \citep{song2024shortcutlearningincontextlearning}.\\

\noindent Instinctive shortcuts can be categorized as follows:
\begin{itemize}
    \item \textit{Vanilla-label bias}: the model exhibits a preference for labels corresponding to high-frequency words in the training corpus;
    \item \textit{Context-label bias}: the model’s predictions are influenced by prompt formatting, including the punctuation used in the instruction and the ordering of demonstrations;
    \item \textit{Domain-label bias}: the model relies on semantic prior knowledge related to the task, showing a bias toward in-domain terms when selecting labels;
    \item \textit{Reasoning-label bias}: the model tends to avoid reasoning over underlying causal relations in Question Answering or other tasks requiring multiple logical steps, instead deriving superficial answers directly from the examples.\\
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{assets/Shortcut Types.png}
    \caption{Classification of LLM shortcuts in the In-Context Learning (ICL) paradigm as presented in \citep{song2024shortcutlearningincontextlearning}}
    \label{fig:types}
\end{figure}

\noindent Acquired shortcuts, in contrast, can be grouped as:
\begin{itemize}
    \item \textit{Lexicon}: reliance on correlations between lexical features in the contextual examples and specific labels (e.g. negation words often associated with the label "contradiction" in the Natural Language Inference, or NLI, task);
    \item \textit{Concept}: internalization of disproportionate co-occurrences between specific concepts and labels (e.g. plants more frequently associated with positive sentiment due to their prevalence in positive examples);
    \item \textit{Overlap}: dependence on word overlapping in text fragments across multiple task branches (e.g. predicting "entailment" in NLI when sentences are repeated between premise and hypothesis);
    \item \textit{Position}: reliance on positional information as a cue for prediction (e.g. options ranked first or last in Multiple-Choice Question Answering are more likely to be chosen);
    \item \textit{Text Style}: association of stylistic cues (e.g. Shakespearean or biblical language) with specific labels;
    \item \textit{Group Dynamics}: influence of relative frequency or co-occurrence patterns among options in multi-choice settings (e.g. if answer "A" appears more often in demonstrations, the model becomes more likely to select "A" than "B").\\
\end{itemize}

Although instinctive and acquired shortcuts arise from different sources, they often interact, as pre-existing tendencies can amplify the impact of spurious cues in demonstrations. The present study represent an attempt to mitigate the effect of acquired shortcuts, since these are directly influenced by prompt design and can thus be more effectively controlled at inference time.

\subsection{Detection and mitigation}
\label{sec:detandmit}

Several lines of research have been proposed in literature to detect the presence of shorcut learning and to mitigate its effects (Figure \ref{fig:detandmit}). A straightforward strategy to reveal non-robust decision rules leveraged by LLMs is to evaluate them on OOD test sets using standard NLP metrics (e.g. accuracy, F1 score), which typically exposes a significant performance degradation \citep{yang2023gluexevaluatingnaturallanguage}. The traditional IID train-test split, in fact, encodes similar biases in both sets and can therefore lead to misleading conclusions about model generalization. 
Adversarial attacks, such as those measuring the sensitivity of LLMs to small input perturbations \citep{jin2020bertreallyrobuststrong}, also provide an effective means to detect potential shortcuts. Moreover, randomization ablation methods can assess whether an LLM truly exploits essential linguistic information in the input (e.g. word order or syntax) or instead ignores it while maintaining high performance due to spurious cues \citep{pham2021orderimportantsequentialorder}. 
Another active line of research relies on explainability techniques, such as feature attribution (e.g. Integrated Gradients) or istance attribution methods \citep{han2020explainingblackboxpredictions}, to identify reliance on artifacts present in the data. It has been shown, for instance, that in Natural Language Understanding (NLU) tasks, models often concentrate their attention on a few tokens belonging to the head of the so-called "long tailed distribution" of words in the training corpus \citep{du2021interpretingmitigatingshortcutlearning}.\\

Existing works on shortcut learning mitigation can be categorized into \textit{data-centric}, \textit{model-centric} and, in the case of ICL settings, \textit{prompt-centric} approaches. Data-centric approaches focus on refining the data on which LLMs are trained in order to minimize opportunities for shortcut exploitation. This can involve filtering out samples with high co-occurrence probabilities, instructing crowd workers to avoid annotation artifacts, or generating new high-quality synthethic data. Data augmentation techniques may also help reduce reliance on shortcuts. However, such approaches can only mitigate a limited subset of biases that are humanly recognizable. Other data-centric strategies include reweighting training samples to assign greater importance to hard examples where the model shows low confidence, thereby improving robustness \citep{utama-etal-2020-towards}, or partitioning the dataset into multiple non-IID subsets (called "training environments"), each containing different spurious correlations, and then maximizing the similarity between gradients across environments, as reliable features tend to be shared among them \citep{shi2021gradientmatchingdomaingeneralization}. Despite their effectiveness, data-centric approaches require retraining or fine-tuning the LLM, which is computationally expensive and may inadvertently alter internal knowledge representations, potentially leading to catastrophic forgetting of useful information \citep{li2024examiningforgettingcontinualpretraining}.\\

Model-centric approaches, on the other hand, concentrate on explicitly or implicitly preventing a model from learning non-robust feature associations. These methods can involve specialized training schemes, manipulation of hidden representations or calibrations of the predictive probability distribution while keeping model parameters frozen.\\

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{assets/big_mitigation_types.png}
    \caption{Classification of shortcut detection and mitigation strategies with methods from \citep{song2024shortcutlearningincontextlearning} and \citep{du2023shortcutlearninglargelanguage}}
    \label{fig:detandmit}
\end{figure}

Various forms of adversarial training can be employed to debias a model. For instance, ensemble adversarial training in the NLP domain can be performed by introducing a task classifier that optimizes the task objective while simultaneously limiting the performance of one or more adversarial classifiers whose goal is to detect artifacts in the training data. Both classifiers share the same encoder, enabling the adversarial component to recognize biases encoded in the original model's representations \citep{stacey-etal-2020-avoiding}. When the shortcuts to which a model is vulnerable are known in advance, explanation regularization offers an effective solution: it reguralizes training by encouraging the model to attend to input feature annotated as important by humans, thereby promoting the learning of rationales and effectively improving OOD performance \citep{stacey2022supervisingmodelattentionhuman}. Another promising technique is the Product-of-Expert approach, which builds a debiased model by enforcing its predictions to be orthogonal to those of a biased model explicitly trained to rely on dataset biases, with the biased model's parameters kept frozen during joint training \citep{clark2019donteasywayout}. Other strategies include confidence regularization, which increases model uncertainty for biased samples via techniques such as knowledge distillation from a biased teacher model \citep{du2021interpretingmitigatingshortcutlearning}, and contrastive learning frameworks that leverage pairs of factual and counterfactual examples (generated, for instance, by masking non-causal and causal terms in the input text \citep{choi2022}) to promote causal reasoning and robustness.\\

A line of research on model-centric shortcut mitigation focuses on model pruning techniques, where the model's hidden units are analyzed and manipulated to reduce the bias encoded within them. Interpretability methods such as Integrated Gradients and Activation Patching are employed to establish a relationship between attention heads or feeed-forward neurons and specific knowledge representations in transformer models, allowing researchers to deactivate biased units \citep{zhou2024unibiasunveilingmitigatingllm}. Another group of approaches involves modifying the output probability distribution of LLMs through various forms of calibration. In ICL settings, for instance, contextual bias can be estimated by measuring the distribution shift of model predictions on meaningless inputs with respect to a uniform random distribution, and then adjusting predictions accordingly. Other calibration strategies include estimating prototypical clusters for each classification category and calibrating predictions based on their likelihood, or leveraging label bias estimated from random in-domain words sampled from the task corpus \citep{song2024shortcutlearningincontextlearning}.\\

Finally, prompt-centric approaches for ICL settings focus on modifying contextual prompts to reduce an LLM's reliance on shortcuts. \textit{Shortcut-based} methods involve masking shortcut words by replacing them with placeholders representing broader conceptual sets, while \textit{instruction format-based} methods alter prompt formatting or task instructions to introduce richer semantic information or mitigate, for instance, option-order effects via majority voting over different permutations. Other approaches employ Chain-of-Thought prompting to guide LLMs towards more coherent and logically grounded predictions \citep{yuan2024llmsovercomeshortcutlearning}. Lastly, \textit{prompt search-based} methods explore the space of possible prompts to identify those that minimize bias, for instance by retrieving particularly effective demonstration examples from the training corpus or by evaluating candidate prompts using entropy-based metrics, since predictions that are overly sensitive to small perturbations are less likely to be correct \citep{song2024shortcutlearningincontextlearning}.

\end{document}