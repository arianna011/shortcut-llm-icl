\documentclass[../main.tex]{subfiles}
% !TeX root = ../main.tex
\graphicspath{{\subfix{../assets/}}}
\begin{document}

\chapter*{Conclusion}

This thesis investigated the problem of shortcut learning in Large Language Models (LLMs) under the In-Context Learning (ICL) paradigm and proposed a training-free, interpretable framework for detecting and mitigating shortcut reliance directly at the level of internal representations. Shortcut learning poses a significant challenge to the robustness, generalization, and interpretability of modern LLMs, as it leads models to exploit superficial or spurious correlations in the input rather than engaging in semantically grounded reasoning. Addressing this phenomenon is particularly important in ICL settings, where models must infer task structure from a small number of examples and are therefore especially susceptible to shortcut cues present in prompts.\\

The core contribution of this work is the adaptation of the Representation Engineering (RepE) framework to the problem of shortcut detection and mitigation. By leveraging contrastive prompt pairs that differ only in the presence of a shortcut cue, the proposed approach extracts latent shortcut-aligned directions from the modelâ€™s hidden representations using simple linear methods such as PCA and ClusterMean. These directions are then manipulated at inference time through RepControl interventions (linear combination, piecewise-linear modulation and projection) allowing shortcut-related activations to be selectively amplified or suppressed without modifying model weights or requiring additional training.\\

Extensive quantitative experiments demonstrated that this approach is effective in mitigating shortcut-driven behavior. Across a diverse set of NLP benchmarks including textual entailment, commonsense reasoning, sentiment analysis, word sense disambiguation and multi-domain question answering, RepControl consistently improved performance over the baseline model. Notably, these improvements were observed even if the shortcut direction was extracted exclusively from textual entailment data, suggesting that shortcut reliance may reflect a more general inductive bias shared across tasks rather than a purely task-specific artifact. The results further showed that no single control operator universally dominates, highlighting the task-dependent nature of shortcut mitigation and the importance of carefully selecting both the intervention strength and the layers at which it is applied.\\

At the same time, experiments combining multiple shortcut directions through a simple normalized aggregation strategy showed that naive joint mitigation can degrade performance, performing worse than both the baseline model and single-shortcut interventions. This finding highlights a key limitation of current linear representation-level control methods and suggests that different shortcut types, while related, are not trivially composable in representation space.\\

Qualitative analyses complemented the quantitative findings and provided additional interpretability insights. Controlled generations revealed that suppressing shortcut-aligned directions can alter model decisions in the expected way, while amplifying them can reliably bias predictions toward shortcut-driven outcomes. Token-level attribution analyses based on Integrated Gradients further showed that RepControl reduces the importance assigned to shortcut-related lexical cues and partially redistributes attention toward more semantically meaningful tokens. At the same time, these analyses exposed limitations of the approach, including residual noise in attribution patterns and incomplete suppression of shortcut effects, underscoring the entangled and highly superposed nature of LLM representations.\\ 

To better understand these limitations, the thesis also investigated the intrinsic dimensionality of shortcut-related representations across layers using both linear (PCA-based) and non-linear (TwoNN) estimators. The results revealed a substantial gap between linear and non-linear dimensionality estimates, indicating that shortcut-prone representations often lie on curved, low-dimensional manifolds embedded within high-dimensional spaces. This finding provides a principled explanation for why linear methods such as PCA are effective but imperfect: while they can capture dominant shortcut directions, they cannot fully disentangle more complex, non-linear shortcut mechanisms.\\

Overall, this work demonstrates that shortcut learning in LLMs can be meaningfully analyzed and mitigated through representation-level interventions, offering a lightweight and interpretable alternative to retraining-based approaches. At the same time, it exposes important open challenges. Shortcut mechanisms appear to be distributed, partially non-linear and entangled with task-relevant features, limiting the effectiveness of single-direction linear control. Future research directions include the development of non-linear or multi-directional representation controls, adaptive or learned aggregation strategies for multiple shortcuts and automated methods for constructing high-quality contrastive datasets. More broadly, the results support the view that understanding and steering the internal representations of LLMs is a promising path toward more robust, transparent and trustworthy language models.

\subbib{}
\end{document}